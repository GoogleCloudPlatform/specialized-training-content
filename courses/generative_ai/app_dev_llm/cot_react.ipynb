{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ5caKL2Ff2B"
   },
   "source": [
    "# Advanced Prompting: Chain of Thought and ReAct (Reasoning + Acting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMkREhcA-Rtw"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pecYSnz2i2fk"
   },
   "source": [
    "This notebook is based off the **Chain of Thought and ReAct (Reasoning + Acting)** notebook found in the [applied-ai-engieering-examples](https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples) GitHub repository. This repository contains reference guides, blueprints, code samples, and hands-on labs developed by the Google Cloud Applied AI Engineering team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H106E0clf7t"
   },
   "source": [
    "# Part 0: Introduction\n",
    "\n",
    "The target audience of this notebook are engineering prompts to repeatedly execute a task, workflow, process, function, etc. Stability and performance are more important than when prompting for a one-off need.\n",
    "\n",
    "This notebook covers two powerful LLM prompting strategies: **Chain of Thought** and **ReAct** (Reasoning + Acting).\n",
    "\n",
    "ReAct (and its variants) are the current state-of-the-art prompting technique to improve LLM reasoning while minimizing hallucinations.\n",
    "\n",
    "The four parts of this notebook are:\n",
    "\n",
    "1. Chain-of-Thought Prompting: Using language descriptions of reasoning to improve LLM outputs.\n",
    "1. Actions, Retrieval, and Tool Use: How LLMs interact with external systems.\n",
    "1. ReAct (Reasoning + Acting) Prompting: Combining the written reasoning descriptions of chain-of-thought prompting with external system interactions.\n",
    "\n",
    "This notebook was tested in Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbz5Q4flkDgo"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "- An understanding of LLMs (large language models):\n",
    " - What an LLM is and how they work.\n",
    " - LLMs as repetitive next-token predictors.  \n",
    " - LLM predictions maximize resemblance to the training data.\n",
    "- Experience with LLM prompting:\n",
    " - What it means to \"prompt\" a language model. [Recommended resource](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/introduction-prompt-design).\n",
    " - The difference between [zero-shot, one-shot, and few-shot](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/introduction-prompt-design#include-examples) prompting, and an understanding why few-shot prompting is essential for maximizing performance and robustness.\n",
    "- Basic familiarity with Google Cloud Vertex LLMs. [Recommended resource](https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/api-quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmWgaCsdu6k1"
   },
   "source": [
    "## Key Terminology\n",
    "\n",
    "For consistency this notebook uses the following terms in specific ways:\n",
    "\n",
    "* **Prompt**: A templated LLM call, created using specific techniques that maximize the performance and robustness of the call regardless of what values are inserted into the template.\n",
    "* **LLM Call**: Sending text to an LLM.\n",
    "* **LLM Response**: Text predicted by the LLM, what comes back from the LLM when making an LLM call.\n",
    "* **Chain/Chaining** Depending on context:\n",
    " * In chain-of-thought prompting, logically sequential steps of reasoning.\n",
    " * In LLM systems, sequential calls to an LLM, where each call depends on a previous call's response.\n",
    "* **Exemplar**: An \"example\" in a one- or few-shot prompt.\n",
    " * Used to avoid confusion with \"example\" in the traditional ML sense, i.e., \"a piece of data\" (as in \"training examples\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-glBTWPl1WD"
   },
   "source": [
    "## References\n",
    "\n",
    "* Kojima, Takeshi, et al. \"Large language models are zero-shot reasoners.\" Advances in neural information processing systems 35 (2022): 22199-22213. [Link](https://arxiv.org/abs/2205.11916) (accessed 2023 09 22)\n",
    "* Wang, Xuezhi, et al. \"Self-consistency improves chain of thought reasoning in language models.\" arXiv preprint arXiv:2203.11171 (2022). [Link](https://arxiv.org/abs/2203.11171) (accessed 2023 09 03).\n",
    "* Wei, Jason, et al. \"Chain-of-thought prompting elicits reasoning in large language models.\" Advances in Neural Information Processing Systems 35 (2022): 24824-24837. [Link](https://arxiv.org/abs/2201.11903) (accessed 2023 09 03).\n",
    "* Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\" arXiv preprint arXiv:2210.03629 (2022). [Link](https://arxiv.org/abs/2210.03629) (accessed 2023 09 03)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC1b7po9xWM6"
   },
   "source": [
    "## Setup -- Run This Code First!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NZ_4h24m-B8u",
    "outputId": "639a80ce-26a2-47e3-c992-b3032338d33b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tested with these package versions.\n",
    "!pip install --user google-cloud-aiplatform==1.71.1 prettyprinter==0.18.0 wikipedia==1.4.0 numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCngWdptsN_Q"
   },
   "source": [
    "**MAKE SURE TO RESTART YOUR RUNTIME BEFORE GOING FURTHER**\n",
    "\n",
    "As long the runtime isn't deleted (even if it restarts) you don't need to re-run this previous cell.\n",
    "\n",
    "Rerun the remaining cells in part 0 if your runtime restarts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSeWZt3ZpxeY"
   },
   "source": [
    "**Set your Google Cloud project ID in the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fTAg64qFY2B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"YOUR-PROJECT-ID\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "# Code examples may misbehave if the model is changed.\n",
    "MODEL_NAME = \"gemini-1.5-pro\"\n",
    "\n",
    "# Set up Vertex AI.\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "vertexai.init(project=PROJECT_ID,\n",
    "              location=LOCATION)\n",
    "parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "\n",
    "model = GenerativeModel(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSpDXdhBvhtu"
   },
   "source": [
    "This function is used throughout the notebook to show the full LLM call and the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esxRVsLAvvr6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "def call_llm(model, parameters, llm_call, show_activity=True, max_retries=5):\n",
    "    \n",
    "    generation_config = GenerationConfig(**parameters)\n",
    "    \n",
    "    attempt = 1\n",
    "    \n",
    "    while attempt <= max_retries:\n",
    "    \n",
    "        try:\n",
    "            output = model.generate_content(llm_call, generation_config=generation_config)\n",
    "            response = str(output.candidates[0].content.parts[0]).split(\"text:\")[1]\n",
    "\n",
    "            if show_activity:\n",
    "                BOLD = \"\\033[1m\"\n",
    "                UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
    "                print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
    "                print(f\"{BOLD}The response:{UNFORMAT}\\n{response}\")\n",
    "\n",
    "            return response  # Return to `_` if not needed.\n",
    "\n",
    "        except ResourceExhausted as e:\n",
    "            time.sleep(2**attempt)\n",
    "            attempt += 1\n",
    "            if attempt > max_retries:\n",
    "                raise ResourseExhausted(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoiMSEJoY9gt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap code cell output to improve notebook readability.\n",
    "# Source: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results/61401455#61401455\n",
    "from IPython.core.formatters import BaseFormatter\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "class MultilineStringFormatter(BaseFormatter):\n",
    "    def __call__(self, obj):\n",
    "        if isinstance(obj, str) and '\\n' in obj:\n",
    "            return f'<pre>{obj}</pre>'\n",
    "        return None\n",
    "\n",
    "# Register the custom formatter\n",
    "ip = get_ipython()\n",
    "ip.display_formatter.formatters['text/html'].for_type(str, MultilineStringFormatter())\n",
    "\n",
    "def set_css(arg):\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "US-jQm1MuGBa"
   },
   "source": [
    "# Part 1: Chain-of-Thought Prompting\n",
    "\n",
    "To LLMs, chains are more than a fashionable accessory.\n",
    "\n",
    "<img src=\"./images/1-chains.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82YfCjFJVX60"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In chain-of-thought prompting, you provide one- or few-shot exemplars showing the reasoning steps to get to a desired output. This is different from standard one- or few-shot prompting, where your exemplars show only the input and the correct output.\n",
    "\n",
    "The reasoning breakdown you provide in chain-of-thought exemplars is similar to the natural language internal monologue a person has as they think through a problem or task.\n",
    "\n",
    "If \"internal monologue\" is a strange concept, think about how you verbalize your thoughts to solve a problem or accomplish a task. For example, you're cooking dinner:\n",
    "\n",
    " ```Ok I've chopped the celery. Now I need to get started on the chicken. Is the oven on? Let me start preheating the oven. Wait, what temperature? I need to check the recipe again...```\n",
    "\n",
    "This \"internal monologue\" or \"inner speech\" facilitates applying problem solving patterns to new problems we haven't seen before, by identifying what should happen next to make progress on the task.\n",
    "\n",
    "By calling the LLM with exemplars that include an \"internal monologue\" of text reasoning, the LLM produces responses that include similar text reasoning. Having the LLM generate the reasoning text as part of the response increases the chance the response ends with the desired output.\n",
    "\n",
    "The reasoning steps in the response\n",
    " also provide interpretability of how the LLM arrived at the final output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydRfjsuBI5Ip"
   },
   "source": [
    "## Chain of Thought Basics\n",
    "\n",
    "Math word problems are a good chain-of-thought demonstration, since they are simple mathematically and logically but require multiple steps of reasoning.\n",
    "\n",
    "In this example (from the Chain of Thought [paper](https://arxiv.org/pdf/2201.11903.pdf)) note the incorrect answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "0VJcAD7lYXE0",
    "outputId": "188ae075-ff00-4a5b-fca2-b40ec449a777",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: The answer is 11.\n",
    "Q: The cafeteria had 23 apples.\n",
    "If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "A:\"\"\"\n",
    "\n",
    "_ = call_llm(model, parameters, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vmzEro2Z707"
   },
   "source": [
    "Rewriting the exemplar to include a chain of thought shows the LLM how to decompose the question into multiple simple steps of reasoning.\n",
    "\n",
    "The model response then follows a similar chain of thought, increasing the likelihood of a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "X_QojLuvZzLV",
    "outputId": "9f88591f-1f86-4092-8d22-63a875cb0df0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
    "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples.\n",
    "If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "A:\"\"\"\n",
    "\n",
    "_ = call_llm(model, parameters, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjwgFMOLaem9"
   },
   "source": [
    "Notice the chain of thought includes both text describing the steps to follow and intermediate outputs/conclusions from each reasoning step.\n",
    "\n",
    "Try experimenting with different questions by changing the `question` variable in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "Fd4e62T7aWoG",
    "outputId": "6861171f-a05a-4606-af7a-b959dd67b4bb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"\"\"Nomfundo writes legal briefs.\n",
    "Each brief has 3 sections, each section takes 4 hours.\n",
    "She wrote 3 briefs this week. How long did it take?\"\"\"\n",
    "\n",
    "one_shot_exemplar = \"\"\"Q: Roger has 5 tennis balls.\n",
    "He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
    "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: \"\"\"\n",
    "\n",
    "# Prepending the one shot exemplar before the question we want answered.\n",
    "llm_call = f\"{one_shot_exemplar}{question}\\nA:\"\n",
    "_ = call_llm(model, parameters, llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XUp7beLcQsS"
   },
   "source": [
    "The LLM response will usually mimic the reasoning style in the exemplars. This means you'll get the best performance if the chain of thought reasoning in your exemplars is a good fit for the task.\n",
    "\n",
    "Compare the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "BPQVYIPucnkF",
    "outputId": "42377444-7537-4300-bcb7-a92bd896565f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct answer: 360, 375.\n",
    "question = \"\"\"A high efficiency factory produces 100 units per day.\n",
    "A medium efficiency factory produces 60 units per day.\n",
    "A low efficiency factory produces 30 units per day.\n",
    "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
    "Tomorrow they reconfigure a low efficiency factory up to medium efficiency.\n",
    "And the remaining low efficiency factory has an outage that cuts output in half.\n",
    "How many units can they produce today? How many tomorrow?\"\"\"\n",
    "\n",
    "one_shot_exemplar = \"\"\"Q: Roger has 5 tennis balls.\n",
    "He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
    "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: \"\"\"\n",
    "\n",
    "llm_call = f\"{one_shot_exemplar}{question}\\nA:\"\n",
    "_ = call_llm(model, parameters, llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJ6Xo0gwpi35"
   },
   "source": [
    "Note the mistake in the output. The LLM response fails to account for the 3 high efficiency factories that are still running tomorrow.\n",
    "\n",
    "For this task, it's better to use a chain of thought with reasoning steps that include a connection to different units of measurement (tennis ball can sizes vs. factory outputs) along with a carrying over of counts between days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "ThikEZV1cNYM",
    "outputId": "5f3249ea-d624-4777-9729-00b58a6ab509",
    "tags": []
   },
   "outputs": [],
   "source": [
    "better_one_shot_exemplar = \"\"\"Q: A large tennis ball can has 5 balls.\n",
    "A small tennis ball can has 3 balls.\n",
    "Roger has 3 large cans and 2 small cans today.\n",
    "Tomorrow he wins a bet and turns one small can into a large can.\n",
    "How many balls does he have today? How many tomorrow?\n",
    "A: 3 large cans is 3 * 5 = 15 tennis balls.\n",
    "2 small cans is 2 * 3 = 6 tennis balls.\n",
    "Today Roger has 15 + 6 = 21 tennis balls.\n",
    "Tomorrow's trade means losing one small tennis ball can and gaining a large can.\n",
    "Roger still has the cans he had yesterday.\n",
    "2 small cans from yesterday - 1 = 1 small can\n",
    "3 large cans from yesterday + 1 = 4 large cans\n",
    "4 large cans is 4 * 5 = 20 tennis balls.\n",
    "1 small can is 1 * 3 tennis balls.\n",
    "Tomorrow Roger has 20 + 3 = 23 tennis balls.\n",
    "Q: \"\"\"\n",
    "\n",
    "llm_call = f\"{better_one_shot_exemplar}{question}\\nA:\"\n",
    "_ = call_llm(model, parameters, llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXNKuX_BttIk"
   },
   "source": [
    "## Chain of Thought Use Cases\n",
    "\n",
    "Math word problems may not be very useful, but chain of thought works well on other types of problems.\n",
    "\n",
    "Some examples from the chain of thought [paper](https://arxiv.org/pdf/2201.11903.pdf) are manipulating information, assessing plausibility, giving instructions, altering/understanding text, and tracking state:\n",
    "\n",
    "<img src=\"./images/2-cot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yX-kn_08m6VW"
   },
   "source": [
    "Other types of tasks that respond well to chain of thought are:\n",
    "* Transforming and enriching data.\n",
    "* Interpreting data.\n",
    "* Code generation.\n",
    "* Evaluating the quality of text (including evaluating the quality of LLM responses).\n",
    "* Creating synthetic data.\n",
    "\n",
    "Generally, any kind of problem that is solved by \"talking through\" a few simple steps is a good chain of thought candidate.\n",
    "\n",
    "For more complex chain of thought usage, the more consistent your chain-of-thought reasoning style across your exemplars, the more likely the LLM follows that same style of reasoning in its response. Note this in the next two examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRwGi1BUX8IE"
   },
   "source": [
    "#### Example: Table Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "vFFmFWgIw_Lt",
    "outputId": "cb69607c-8aa8-4ef6-e12e-63c9970b3d0d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The correct answer is Post-War British Literature.\n",
    "question = \"\"\"\n",
    "| Book Name | Edition | ISBN | Publisher | Aug 1 Amazon Avg New Price | Aug 1 Amazon Avg Used Price | Aug 1 Abebooks Avg New Price | Aug 1 Abebooks Avg Used Price | Sep 1 Amazon Avg New Price | Sep 1 Amazon Avg Used Price | Sep 1 Abebooks Avg New Price | Sep 1 Abebooks Avg Used Price |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| Physics for Computer Scientists | 10th | 978-1-118-56906-1 | Pearson Education | $149.99 | $79.99 | $142.94 | $66.94 | $129.99 | $59.99 | $139.94 | $56.94 |\n",
    "| Fundamentals of Calculus | 8th | 978-0-470-45831-0 | John Wiley & Sons | $139.99 | $99.99 | $137.94 | $87.94 | $129.99 | $79.99 | $129.94 | $76.94 |\n",
    "| Post-War British Literature | 2nd | 978-0-300-08897-2 | Oxford University Press | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
    "| Modern Religions: An Overview | 3rd | 978-0-19-992545-3 | Oxford University Press | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
    "| The Norton Introduction to Literature | 11th | 978-0-393-45078-1 | W. W. Norton & Company | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
    "| The Norton Anthology of American Literature | 9th | 978-0-393-93750-8 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
    "| The Norton Anthology of World Literature | 8th | 978-0-393-92855-6 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
    "| The Elements of Style | 5th | 978-0-205-11265-3 | Longman | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
    "\n",
    "What Oxford book dropped the most in used book price on Amazon between Aug and Sep?\n",
    "\"\"\"\n",
    "\n",
    "context = \"\"\"Answer questions about a table.\n",
    "All questions must be supported by facts in the table.\n",
    "All reasoning must be done step by step.\n",
    "Explain the reasoning.\n",
    "When looking at multiple rows, explain the reasoning for each row one by one.\n",
    "\"\"\"\n",
    "\n",
    "llm_call = f\"{context}\\n{question}\\nAnswer:\"\n",
    "_ = call_llm(model, parameters, llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_bpOTJcXviZ"
   },
   "source": [
    "Now we add a few exemplars.\n",
    "\n",
    "Note that the exemplars use a different source table than the question, but the chain-of-thought reasoning still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SGUOqCKO_SIW",
    "outputId": "7da90243-6768-4891-fd6c-a97ff35e565d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_shot_exemplar = \"\"\"\n",
    "Table:\n",
    "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
    "|---|---|---|---|---|---|\n",
    "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
    "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
    "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
    "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
    "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
    "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
    "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
    "\n",
    "Question:\n",
    "What iPhone sold the most in August?\n",
    "Answer: I need to look at each item one by one and determine if it is an iPhone.\n",
    "Only iPhone items are considered.\n",
    "The iPhone items are the iPhone 13 Pro Max, the iPhone 13 Pro, and the iPhone 13.\n",
    "I need to look at how much each iPhone sold one by one, and then see which sold count is the highest.\n",
    "iPhone 13 Pro Max sale count is 17.\n",
    "iPhone 13 Pro sale count is 9.\n",
    "iPhone 13 sale count is 4.\n",
    "The biggest number of 17, 9, and 4 is 17.\n",
    "The answer is iPhone 13 Pro Max.\n",
    "\n",
    "Table:\n",
    "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
    "|---|---|---|---|---|---|\n",
    "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
    "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
    "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
    "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
    "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
    "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
    "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
    "\n",
    "Question:\n",
    "What Samsung phone has the most units unaccounted for on Sep 1?\n",
    "Answer: I need to look at each item one by one and determine if it is a Samsung item.\n",
    "I have to look at the Item Name for Samsung items.\n",
    "Only Samsung items are considered.\n",
    "The Samsung items are the S22 Ultra, the S22 Plus, and the S22.\n",
    "One by one, I need to look at the Sep 1 and Aug 1 inventory difference for each Samsung item to see how many units should have been sold.\n",
    "Then I need to compare that number to the actual sale count value for that item.\n",
    "The phone with the biggest difference between the sale count field and the inventory differences is the most unaccounted for.\n",
    "Samsung Galaxy S22 Ultra had 100 in stock Aug 1 and 80 in stock Sep 1. 100 minus 80 is 20 (100 - 80 = 20). Sale count is 19. 20 minus 19 is 1 (20 - 19 = 1). 1 unit is unaccounted for.\n",
    "Samsung Galaxy S22 Plus had 50 in stock Aug 1 and 40 in stock Sep 1. 50 minus 40 is 10 (50 - 40 = 10). Sale count is 10. The sale count matches the inventory difference, no units are unaccounted for.\n",
    "Samsung Galaxy S22 had 25 in stock Aug 1 and 20 in stock Sep 1. 25 minus 20 is 5 (25 - 20 = 5). Sale count is 5. 20 minus 19 is 1. The sale count matches the inventory difference, no units are unaccounted for.\n",
    "Only the S22 Ultra had anything unaccounted for.\n",
    "The answer is Samsung Galaxy S22 Ultra.\n",
    "\n",
    "Table:\n",
    "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
    "|---|---|---|---|---|---|\n",
    "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
    "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
    "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
    "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
    "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
    "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
    "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
    "\n",
    "Question:\n",
    "What vendor had the most total sales?\n",
    "Answer: I need to look at the vendors one by one.\n",
    "I have to deduce the vendors from the Item Name field.\n",
    "There are three unique vendors in the table: Apple, Samsung, and Google.\n",
    "For each vendor, I need to find the sale count for each item one by one, then add up the sales counts.\n",
    "The Apple items are the iPhone 13 Pro Max with 17 sales, the iPhone 13 Pro with 9 sales, and the iPhone 13 with 4 sales.\n",
    "17 + 9 + 4 = 30. 30 Apple phones were sold.\n",
    "The Samsung items are the Samsung Galaxy S22 Ultra with 19 sales, the Samsung Galaxy S22 Plus with 10 sales, and the Samsung Galaxy S22 with 5 sales.\n",
    "19 + 10 + 5 = 34. 34 Samsung phones were sold.\n",
    "The Google item is the Google Pixel 6 Pro with 20 sales. 20 Google phones were sold.\n",
    "30 Apple, 34 Samsung, 20 Google. 34 is the biggest number, it is for Samsung sales.\n",
    "The answer is Samsung.\n",
    "\n",
    "Table:\n",
    "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
    "|---|---|---|---|---|---|\n",
    "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
    "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
    "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
    "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
    "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
    "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
    "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
    "\n",
    "Question:\n",
    "What item had the most sales?\n",
    "Answer: I need to look at each item one by one.\n",
    "The iPhone 13 Pro Max had 17 sales.\n",
    "The iPhone 13 Pro had 9 sales.\n",
    "The iPhone 13 had 4 sales.\n",
    "The Samsung Galaxy S22 Ultra had 19 sales.\n",
    "The Samsung Galaxy S22 Plus had 10 sales.\n",
    "The Samsung Galaxy S22 had 5 sales.\n",
    "The Google Pixel 6 Pro had 20 sales.\n",
    "The sales numbers are 17, 9, 3, 19, 10, 5, and 20.\n",
    "20 is the biggest sales number, that is for the Google Pixel 6 Pro.\n",
    "The answer is the Google Pixel 6 Pro.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Prepending the few shot exemplars before the question we want answered.\n",
    "llm_call = f\"{context}\\n{few_shot_exemplar}{question}\\nAnswer:\"\n",
    "_ = call_llm(model, parameters, llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf0vyGCAZndK"
   },
   "source": [
    "Two more questions (suppressing the model call for readability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "Dm_GnH8yZb9-",
    "outputId": "9dbed298-b391-46e7-e959-5fbff4cff122",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The correct answer is $6.15.\n",
    "question = \"\"\"\n",
    "Table:\n",
    "| Book Name | Edition | ISBN | Publisher | Aug 1 Amazon Avg New Price | Aug 1 Amazon Avg Used Price | Aug 1 Abebooks Avg New Price | Aug 1 Abebooks Avg Used Price | Sep 1 Amazon Avg New Price | Sep 1 Amazon Avg Used Price | Sep 1 Abebooks Avg New Price | Sep 1 Abebooks Avg Used Price |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| Physics for Computer Scientists | 10th | 978-1-118-56906-1 | Pearson Education | $149.99 | $79.99 | $142.94 | $66.94 | $129.99 | $59.99 | $139.94 | $56.94 |\n",
    "| Fundamentals of Calculus | 8th | 978-0-470-45831-0 | John Wiley & Sons | $139.99 | $99.99 | $137.94 | $87.94 | $129.99 | $79.99 | $129.94 | $76.94 |\n",
    "| Post-War British Literature | 2nd | 978-0-300-08897-2 | Oxford University Press | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
    "| Modern Religions: An Overview | 3rd | 978-0-19-992545-3 | Oxford University Press | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
    "| The Norton Introduction to Literature | 11th | 978-0-393-45078-1 | W. W. Norton & Company | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
    "| The Norton Anthology of World Literature | 8th | 978-0-393-92855-6 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
    "| The Elements of Style | 5th | 978-0-205-11265-3 | Longman | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
    "\n",
    "Question:\n",
    "How much money would be saved if I purchased 3 new copies of the Elements of Style from Abe books instead of Amazon in August?\n",
    "\"\"\"\n",
    "\n",
    "llm_call = f\"{context}\\n{few_shot_exemplar}{question}\\nAnswer:\"\n",
    "print(call_llm(model, parameters, llm_call, show_activity=False))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# The correct answer is Physics for Computer Scientists.\n",
    "question = \"\"\"\n",
    "Table:\n",
    "| Book Name | Edition | ISBN | Publisher | Aug 1 Amazon Avg New Price | Aug 1 Amazon Avg Used Price | Aug 1 Abebooks Avg New Price | Aug 1 Abebooks Avg Used Price | Sep 1 Amazon Avg New Price | Sep 1 Amazon Avg Used Price | Sep 1 Abebooks Avg New Price | Sep 1 Abebooks Avg Used Price |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| Physics for Computer Scientists | 10th | 978-1-118-56906-1 | Pearson Education | $149.99 | $79.99 | $142.94 | $66.94 | $129.99 | $59.99 | $139.94 | $56.94 |\n",
    "| Fundamentals of Calculus | 8th | 978-0-470-45831-0 | John Wiley & Sons | $139.99 | $99.99 | $137.94 | $87.94 | $129.99 | $79.99 | $129.94 | $76.94 |\n",
    "| Post-War British Literature | 2nd | 978-0-300-08897-2 | Oxford University Press | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
    "| Modern Religions: An Overview | 3rd | 978-0-19-992545-3 | Oxford University Press | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
    "| The Norton Introduction to Literature | 11th | 978-0-393-45078-1 | W. W. Norton & Company | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
    "| The Norton Anthology of World Literature | 8th | 978-0-393-92855-6 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
    "| The Elements of Style | 5th | 978-0-205-11265-3 | Longman | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
    "\n",
    "Question: What book has the largest difference between new and used Aug Amazon prices?\n",
    "\"\"\"\n",
    "\n",
    "llm_call = f\"{context}\\n{few_shot_exemplar}{question}\\nAnswer:\"\n",
    "print(call_llm(model, parameters, llm_call, show_activity=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jk98xwBpSnl"
   },
   "source": [
    "For a data understanding use case, if you know the data schema ahead of time your exemplars should match that schema.\n",
    "\n",
    "Generally, the more alike in structure the exemplar data structures are to the question data structure, the more likely the LLM responds correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWB4WcfdaLNi"
   },
   "source": [
    "#### Example: Tagging Data and Structured Data Output\n",
    "\n",
    "Two common needs for an LLM workflow are to generate tags or categories from a description, and to output structured data.\n",
    "\n",
    "This example does both. Tagging performance improves with chain-of-thought exemplars that reason through why certain tags are best (and provide interpretability for why the tags were chosen).\n",
    "\n",
    "Additionally, showing what the structured data output should look like, even for a common data format like JSON, will improve performance.\n",
    "\n",
    "[Data source](https://data.amerigeoss.org/dataset/gsa-json-adc1d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "id": "9xOLcvQdXWfd",
    "outputId": "ed0f90a9-3d95-424d-df5f-0769f62c370a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Given a JSON entry of a data source, output a JSON with the following fields and explain the reasoning:\n",
    "pii: True/False, the dataset contains Personally Identifiable Information.\n",
    "age: How many years since the dataset was last modified.\n",
    "keywords: New keywords to index this dataset under, beyond the current set of keywords.\n",
    "The last text output should be the JSON.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "{\n",
    "    \"@type\" : \"dcat:Dataset\",\n",
    "    \"description\" : \"<p>The MDS 3.0 Frequency Report summarizes information for active residents currently in nursing homes. The source of these counts is the residents MDS assessment record. The MDS assessment information for each active nursing home resident is consolidated to create a profile of the most recent standard information for the resident.</p>\\n\",\n",
    "    \"title\" : \"MDS 3.0 Frequency Report\",\n",
    "    \"accessLevel\" : \"public\",\n",
    "    \"identifier\" : \"465\",\n",
    "    \"license\" : \"http://opendefinition.org/licenses/odc-odbl/\",\n",
    "    \"modified\" : \"2016-04-05\",\n",
    "    \"temporal\" : \"2012-01-01T00:00:00-05:00/2015-12-31T00:00:00-05:00\",\n",
    "    \"contactPoint\" : {\n",
    "      \"@type\" : \"vcard:Contact\",\n",
    "      \"fn\" : \"Health Data Initiative\",\n",
    "      \"hasEmail\" : \"mailto:HealthData@hhs.gov\"\n",
    "    },\n",
    "    \"bureauCode\" : [ \"009:38\" ],\n",
    "    \"keyword\" : [ \"Activities of Daily Living (ADL)\" ],\n",
    "    \"language\" : [ \"en\" ],\n",
    "    \"programCode\" : [ \"009:000\" ],\n",
    "    \"publisher\" : {\n",
    "      \"@type\" : \"org:Organization\",\n",
    "      \"name\" : \"Centers for Medicare & Medicaid Services\",\n",
    "      \"subOrganizationOf\" : {\n",
    "        \"@type\" : \"org:Organization\",\n",
    "        \"name\" : \"Department of Health & Human Services\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm_call = f\"{context}\\nJSON:{question}\\nAnswer:\"\n",
    "_ = call_llm(model, parameters, llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0W-zY4uewRs"
   },
   "source": [
    "The JSON format is correct, but age is wrong and no keywords were predicted. Adding one exemplar leads to a correct response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qUn2EeXQe6pu",
    "outputId": "f14170ad-651f-46f2-e1c7-af00b63f7fe1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_shot_exemplar = \"\"\"\n",
    "JSON:\n",
    "{\n",
    "\n",
    "    \"@type\" : \"dcat:Dataset\",\n",
    "    \"description\" : \"The primary purpose of this system of records is to properly pay medical insurance benefits to or on behalf of entitled beneficiaries.\",\n",
    "    \"title\" : \"Medicare Multi-Carrier Claims System\",\n",
    "    \"accessLevel\" : \"restricted public\",\n",
    "    \"dataQuality\" : true,\n",
    "    \"identifier\" : \"b6ffafab-1cfd-42dd-b8cb-7a554efaefa7\",\n",
    "    \"landingPage\" : \"http://www.cms.gov/Research-Statistics-Data-and-Systems/Computer-Data-and-Systems/Privacy/Systems-of-Records-Items/09-70-0501-MCS.html\",\n",
    "    \"license\" : \"http://www.usa.gov/publicdomain/label/1.0/\",\n",
    "    \"modified\" : \"2014-09-30\",\n",
    "    \"rights\" : \"Contains personally identifiable information and is subject to the Privacy Act of 1974, as amended at 5 United States Code (U.S.C.) 552a.  Requests should be directed to the appropriate System Manager, identified in the System of Records notice.\",\n",
    "    \"primaryITInvestmentUII\" : \"009-000004256, 009-000004254\",\n",
    "    \"systemOfRecords\" : \"09-70-0501\",\n",
    "\n",
    "    \"contactPoint\" : {\n",
    "      \"@type\" : \"vcard:Contact\",\n",
    "      \"fn\" : \"Health Data Initiative\",\n",
    "      \"hasEmail\" : \"mailto:Healthdata@hhs.gov\"\n",
    "    },\n",
    "    \"bureauCode\" : [ \"009:38\" ],\n",
    "    \"keyword\" : [ \"medicare\", \"part b\", \"claims\" ],\n",
    "    \"programCode\" : [ \"009:078\" ],\n",
    "    \"theme\" : [ \"Medicare\" ],\n",
    "    \"publisher\" : {\n",
    "      \"@type\" : \"org:Organization\",\n",
    "      \"name\" : \"Centers for Medicare & Medicaid Services\",\n",
    "      \"subOrganizationOf\" : {\n",
    "        \"@type\" : \"org:Organization\",\n",
    "        \"name\" : \"Department of Health & Human Services\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "Answer: The 'rights' tag says 'Contains personally identifiable information' so pii is True.\n",
    "The 'modified' tag is '2014-09-30'. The current year is 2023, 2023 minus 2014 is 9, so the age is 9.\n",
    "To determine keywords I will look at all the fields that describe the dataset.\n",
    "Then I will take the most salient and distinctive aspects of the fields and make those keywords.\n",
    "Looking at all the fields, the ones that describe the dataset are  \"description\" and \"title\".\n",
    "The \"title\" field is \"Medicare Multi-Carrier Claims System\".\n",
    "Good keywords from the \"title\" field are \"medicare\" and \"claims\".\n",
    "The \"description\" field is \"\"The primary purpose of this system of records is to properly pay medical insurance benefits to or on behalf of entitled beneficiaries.\"\n",
    "Good keywords from the \"description\" field are \"medical insurance benefits\".\n",
    "Good proposed keywords from both fields are \"medicare\", \"claims\", and \"medical insurance benefits\".\n",
    "Next inspect the \"keyword\" field to make sure the proposed keywords are not already included.\n",
    "The \"keyword\" field contains the keywords \"medicare\", \"part b\", and \"claims\".\n",
    "From our proposed keywords, \"medicare\" should not be output since it is already in the \"keyword\" field.\n",
    "That leaves \"claims\" and \"medical insurance benefits\" as proposed keywords.\n",
    "\n",
    "Output JSON:\n",
    "{\n",
    "  \"pii\" : true,\n",
    "  \"age\" : 9,\n",
    "  \"keywords\" : [\"claims\", \"medical insurance benefits\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Prepending the one shot exemplar before the question we want answered.\n",
    "llm_call = f\"{context}{one_shot_exemplar}\\nJSON:{question}\\nAnswer:\"\n",
    "_ = call_llm(model, parameters, llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbtSBsrpjg56"
   },
   "source": [
    "The output is correct but the reasoning on keyword overlap could be clearer, which would make the prompt more robust. Think about to improve this, then see the next cell for one solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HIGy06bNkdNf",
    "outputId": "6827d253-c0b9-4d11-ac4d-2c514dae8717",
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_shot_exemplar = \"\"\"\n",
    "JSON:\n",
    "{\n",
    "\n",
    "    \"@type\" : \"dcat:Dataset\",\n",
    "    \"description\" : \"The primary purpose of this system of records is to properly pay medical insurance benefits to or on behalf of entitled beneficiaries.\",\n",
    "    \"title\" : \"Medicare Multi-Carrier Claims System\",\n",
    "    \"accessLevel\" : \"restricted public\",\n",
    "    \"dataQuality\" : true,\n",
    "    \"identifier\" : \"b6ffafab-1cfd-42dd-b8cb-7a554efaefa7\",\n",
    "    \"landingPage\" : \"http://www.cms.gov/Research-Statistics-Data-and-Systems/Computer-Data-and-Systems/Privacy/Systems-of-Records-Items/09-70-0501-MCS.html\",\n",
    "    \"license\" : \"http://www.usa.gov/publicdomain/label/1.0/\",\n",
    "    \"modified\" : \"2014-09-30\",\n",
    "    \"rights\" : \"Contains personally identifiable information and is subject to the Privacy Act of 1974, as amended at 5 United States Code (U.S.C.) 552a.  Requests should be directed to the appropriate System Manager, identified in the System of Records notice.\",\n",
    "    \"primaryITInvestmentUII\" : \"009-000004256, 009-000004254\",\n",
    "    \"systemOfRecords\" : \"09-70-0501\",\n",
    "\n",
    "    \"contactPoint\" : {\n",
    "      \"@type\" : \"vcard:Contact\",\n",
    "      \"fn\" : \"Health Data Initiative\",\n",
    "      \"hasEmail\" : \"mailto:Healthdata@hhs.gov\"\n",
    "    },\n",
    "    \"bureauCode\" : [ \"009:38\" ],\n",
    "    \"keyword\" : [ \"medicare\", \"part b\", \"claims\" ],\n",
    "    \"programCode\" : [ \"009:078\" ],\n",
    "    \"theme\" : [ \"Medicare\" ],\n",
    "    \"publisher\" : {\n",
    "      \"@type\" : \"org:Organization\",\n",
    "      \"name\" : \"Centers for Medicare & Medicaid Services\",\n",
    "      \"subOrganizationOf\" : {\n",
    "        \"@type\" : \"org:Organization\",\n",
    "        \"name\" : \"Department of Health & Human Services\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "Answer: The \"rights\" field says 'Contains personally identifiable information' so pii is true.\n",
    "The \"modified\" field is \"2014-09-30\". The current year is 2023, 2023 minus 2014 is 9, so the age is 9.\n",
    "To determine keywords I will look at all the fields that describe the dataset.\n",
    "Then I will take the most salient and distinctive aspects of the fields and make those keywords.\n",
    "Looking at all the fields, the ones that describe the dataset are \"description\" and \"title\".\n",
    "The \"title\" field is \"Medicare Multi-Carrier Claims System\".\n",
    "Good keywords from the \"title\" field are \"medicare\" and \"claims\".\n",
    "The \"description\" field is \"The primary purpose of this system of records is to properly pay medical insurance benefits to or on behalf of entitled beneficiaries.\"\n",
    "Good keywords from the \"description\" field are \"medical insurance benefits\".\n",
    "Good proposed keywords from both fields are \"medicare\", \"claims\", and \"medical insurance benefits\".\n",
    "Next inspect the \"keyword\" field to make sure the proposed keywords are not already included.\n",
    "The \"keyword\" field contains the keywords \"medicare\", \"part b\", and \"claims\".\n",
    "From our proposed keywords, \"medicare\" should not be output since it is already in the \"keyword\" field.\n",
    "That leaves \"claims\" and \"medical insurance benefits\" as acceptable new keywords.\n",
    "\n",
    "Output JSON:\n",
    "{\n",
    "  \"pii\" : true,\n",
    "  \"age\" : 9,\n",
    "  \"keywords\" : [\"claims\", \"medical insurance benefits\"]\n",
    "}\n",
    "\n",
    "\n",
    "JSON:\n",
    "{\n",
    "  \"@type\": \"dcat:Dataset\",\n",
    "  \"title\": \"Data.gov Top 10 Visiting Countries - Archival\",\n",
    "  \"description\": \"This dataset provides top 10 visiting countries by month in Data.gov up to July 2013.\",\n",
    "  \"modified\": \"2016-01-20\",\n",
    "  \"accessLevel\": \"public\",\n",
    "  \"identifier\": \"GSA-32491\",\n",
    "  \"dataQuality\": true,\n",
    "  \"describedBy\": \"http://www.data.gov/metric\",\n",
    "  \"describedByType\": \"text/csv\",\n",
    "  \"issued\": \"2013-05-13\",\n",
    "  \"license\": \"https://creativecommons.org/publicdomain/zero/1.0/\",\n",
    "  \"spatial\": \"United States\",\n",
    "  \"publisher\": {\n",
    "      \"@type\": \"org:Organization\",\n",
    "      \"name\": \"General Services Administration\"\n",
    "  },\n",
    "  \"accrualPeriodicity\": \"R/P1M\",\n",
    "  \"isPartOf\": \"GSA-2015-09-14-01\",\n",
    "  \"contactPoint\": {\n",
    "      \"@type\": \"vcard:Contact\",\n",
    "      \"fn\": \"Hyon Joo Kim\",\n",
    "      \"hasEmail\": \"mailto:hyon.kim@gsa.gov\"\n",
    "  },\n",
    "  \"distribution\": [{\n",
    "          \"@type\": \"dcat:Distribution\",\n",
    "          \"mediaType\": \"text/csv\",\n",
    "          \"format\": \"text/csv\",\n",
    "          \"title\": \"Data.gov_Top_10_Visiting_Countries.csv\",\n",
    "          \"downloadURL\": \"https://inventory.data.gov/dataset/b0d40da1-a505-476a-a49b-cfc50ea6d9da/resource/0a1a3fb8-a813-4470-b50c-51b7856203be/download/userssharedsdfdata.govtop10visitingcountries.csv\"\n",
    "      }\n",
    "  ],\n",
    "  \"keyword\": [\"Countries\", \"Interactive\"],\n",
    "  \"bureauCode\": [\"023:00\"],\n",
    "  \"programCode\": [\"023:019\"],\n",
    "  \"language\": [\"us-EN\"],\n",
    "  \"theme\": [\"Countries\", \"Top 10\"]\n",
    "  }\n",
    "\n",
    "Answer: The \"accessLevel\" field says \"public\" so pii is False.\n",
    "The \"modified\" field is \"2016-01-20\". The current year is 2023, 2023 minus 16 is 7, so the age is 8.\n",
    "To determine keywords I will look at all the fields that describe the dataset.\n",
    "Then I will take the most salient and distinctive aspects of the fields and make those keywords.\n",
    "Looking at all the fields, the ones that describe the dataset are  \"description\" and \"title\".\n",
    "The \"title\" field is \"Data.gov Top 10 Visiting Countries - Archival\".\n",
    "Good keywords from the \"title\" field are \"data.gov\", \"top 10\".\n",
    "The \"description\" field is \"This dataset provides top 10 visiting countries by month in Data.gov up to July 2013.\"\n",
    "Good keywords from the \"description\" field are \"top 10\" and \"visiting countries\".\n",
    "Good proposed keywords from both fields are \"data.gov\", \"top 10\", and \"visiting countries\".\n",
    "Next inspect the \"keyword\" field to make sure the proposed keywords are not already included.\n",
    "The \"keyword\" field contains the keywords \"Countries\" and \"Interactive\"\n",
    "None of the proposed keywords are in the \"keyword\" field.\n",
    "\"data.gov\", \"top 10\", and \"visiting countries\" are all acceptable new keywords.\n",
    "\n",
    "Output JSON:\n",
    "{\n",
    "  \"pii\" : false,\n",
    "  \"age\" : 9,\n",
    "  \"keywords\" : [\"data.gov\", \"top 10\", \"visiting countries\"]\n",
    "}\n",
    "\"\"\"\n",
    "llm_call = f\"{context}{few_shot_exemplar}\\nJSON:{question}\\nAnswer:\"\n",
    "_ = call_llm(model, parameters, llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKPOkKfax7wB"
   },
   "source": [
    "## Zero-Shot Chain of Thought (\"Let's Think Step by Step\")\n",
    "\n",
    "Zero-shot chain of thought is when you add a \"trigger sentence\" to the end of your LLM call. For example, \"let's think step by step\", \"start by taking a deep breath\", or \"SOLUTION:\". It is a fast and easy way to increase prompt performance and is flexible to different tasks (whereas few-shot chain of thought requires your question resemble the exemplars).\n",
    "\n",
    "However, zero-shot chain of thought underperforms few-shot in almost all situations. Additionally, zero-shot chain of thought requires calling the LLM twice--once to generate the response, and again to extract the  answer from the response (since you don't have exemplars showing the response structure). Finally, zero-shot chain-of-thought has a tendency to restate a question rather than answering it.\n",
    "\n",
    "Generally zero-shot chain-of-thought is not recommended when engineering robust prompts, other than for inspiration when writing few-shot chain-of-thought exemplars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXosOkcbuaTf"
   },
   "source": [
    "## Chain of Thought Advantages\n",
    "\n",
    "1. An easy LLM quality boost for minimal effort.\n",
    "1. Applicable to any task that can be solved by verbally \"talking through\"  the steps to solve a problem.\n",
    "1. Interpretability. This aids debugging and enables use cases that require interpretations for end users.\n",
    "1. Works with off-the-shelf LLMs, no additional LLM training or tuning required.\n",
    "1. Robustness between different LLMs. The final output from chain-of-thought prompts drifts less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqPu2gaXexr3"
   },
   "source": [
    "## Chain of Thought Disadvantages\n",
    "\n",
    "1. Increased cost from longer LLM calls and responses.\n",
    "1. Slower inference times.\n",
    "1. Hallucinations still possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYrjss2N2qnf"
   },
   "source": [
    "## Chain of Thought Best Practices\n",
    "\n",
    "These recommendations reflect current understanding, all things LLM are changing quickly. Some of this will likely be incorrect for  certain corner cases and LLM architectures.\n",
    "\n",
    "If you find exceptions to these best practices, consider filing a Github issue.\n",
    "\n",
    "### Essential Best Practices\n",
    "\n",
    "You must follow these best practices to get the good performance from chain of thought.\n",
    "\n",
    "1. **Don't** Use a small LLM.\n",
    "  * Ideally, use an LLM with at least 15B parameters.\n",
    "  * Expect techniques like distillation and improved LLM architectures to eventually change this advice.\n",
    "1. **Do** Put the answer _after_ the chain-of-thought reasoning, not before.\n",
    "1. **Do** Set [temperature](https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/api-quickstart#try_text_prompts) to 0.\n",
    "1. **Do** Use few-shot chain of thought, not just one-shot or zero-shot.\n",
    "1. **Do** Write exemplars that include everything you would say when talking through the reasoning step-by-step.\n",
    "  * Chain of thought requires natural language reasoning.\n",
    "  * **Don't** Use math equations in place of natural language reasoning. Adding equations to supplement natural language is fine.\n",
    "1. **Don't** Assume chain of thought stops hallucinations.\n",
    " * Chain of thought improves an LLM's ability to reason, but does not stop an LLM from making up facts.\n",
    "\n",
    "### Additional Best Practices\n",
    "\n",
    "More tips to get the most from chain of thought.\n",
    "\n",
    "1. **Don't** Overfocus on the order of few-shot exemplars, it's unlikely to change performance.\n",
    "  * Classification tasks are one exception, don't have too many exeplars of the same class back-to-back.\n",
    "1. **Do** Analyze where your chain-of-thought prompt fails, then craft additional few-shot exemplars to manage common failures.\n",
    "1. **Don't** Write more than six few-shot exemplars to start. Only some tasks  benefit from more.\n",
    "1. **Do** Have multiple prompt engineers each attempt to write the best prompt.\n",
    "  * For example, if you have three tasks to write prompts for and three prompt engineers, anecdotally you'll get better results if each prompt engineer writes prompts for all three tasks vs. each prompt engineer working three times as long on a prompt for a single task.\n",
    "1. **Don't** Expect chain of thought to improve results if your task requires only one or two reasoning steps.\n",
    "1. **Don't** Worry too much about exactly matching the number of reasoning steps in your exemplars vs. your task.\n",
    " * The style or structure of reasoning is more important to match.\n",
    " * There is performance benefit if you _can_ match the number of reasoning steps, but if you can't chain of thought still provides a performance boost.\n",
    "1. **Do** Add chains of thought when tuning an LLM.\n",
    "  * You can prompt an LLM to generate chain-of-thought reasoning from a question and answer, and then add the reasoning to the responses in the tuning data.\n",
    "  * Prompting vs. tuning is a false dichotomy--you'll get the best tuned model performance when tuning data inputs include a well-engineered prompt.\n",
    "1. **Do** Include exemplars that match your data distribution.\n",
    " * For example, if your data is 80% class A and 20% class B and you write 5 few-shot exemplars, 4 exemplars should be class A and 1 should be class B.\n",
    " * With classification tasks the exemplar order can matter, but matching the class distribution increases order robustness.\n",
    " * **Do** make sure not too many back-to-back exemplars are the same class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wY8sKdk9fN3Z"
   },
   "source": [
    "## Self-Consistency\n",
    "\n",
    "Self-Consistency is a technique to improve the performance of chain of thought prompts--you make the same LLM call multiple times and take the most common answer.\n",
    "\n",
    "This means \"breaking\" the rule to use chain of thought with temperature=0.\n",
    "\n",
    "The intuition behind self-consistency is:\n",
    "1. Multiple responses to identical LLM calls means a variety of reasoning paths in the responses.\n",
    "1. Incorrect reasoning paths lead to different incorrect answers.\n",
    "1. Correct reasoning paths lead to the same correct answer.\n",
    "1. While you may only get a few correct answers and many incorrect answers, the correct answer will be more common than any unique incorrect answer.\n",
    "\n",
    "Let's try self-consistency. First, run this next LLM call with temperature 0 to generate an incorrect response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 798
    },
    "id": "pYKVZ8iHhf1d",
    "outputId": "5fb368bd-ad86-47e6-a643-cb9bf840eb1d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The answer is 1300 + 100 (maintenance) + 75 (upgrade) = 1475.\n",
    "question = \"\"\"Factories have a baseline productivity of 100 units per day.\n",
    "Not all factories have the baseline productivity.\n",
    "When a factory is being upgraded, it has 25% of the baseline productivity.\n",
    "When a factory is undergoing maintenance, it has 50% of the baseline.\n",
    "When a factory is under labor action, it produces nothing.\n",
    "Megacorp has 19 factories in total.\n",
    "3 factories are being upgraded.\n",
    "2 factories are under maintenance.\n",
    "1 is under labor action.\n",
    "How many units does megacorp produce in a day?\"\"\"\n",
    "\n",
    "context = \"\"\"Answer questions showing the full math and reasoning.\n",
    "Follow the pattern in the example.\n",
    "\"\"\"\n",
    "\n",
    "one_shot_exemplar = \"\"\"Q: A regular tennis ball can holds 5 balls.\n",
    "A large tennis ball can holds 200% of a regular tennis ball can.\n",
    "A small tennis ball can holds 40% of a regular tennis ball can.\n",
    "A collectable tennis ball can holds no tennis balls.\n",
    "Roger has 10 tennis ball cans.\n",
    "3 cans are large cans.\n",
    "4 cans are small cans.\n",
    "1 can is collectable.\n",
    "How many tennis balls does Roger have?\n",
    "A: We need to find the number of regular tennis ball cans.\n",
    "Roger has 10 (total) - 3 (large) - 4 (small) - 1 (collectable) = 2 regular cans.\n",
    "A large tennis ball can holds 200% of 5 = 10 tennis balls.\n",
    "A small tennis ball can holds 40% of 5 = 2 tennis balls.\n",
    "Next count how many balls come from each can type.\n",
    "3 large cans is 3 * 10 = 30 tennis balls.\n",
    "4 small cans is 2 * 4 = 8 tennis balls.\n",
    "2 regular cans is 2 * 5 = 10 tennis balls\n",
    "1 collectable can is 0 tennis balls.\n",
    "To get the answer, add the number of balls from each can type.\n",
    "Roger has 30 (large) + 8 (small) + 10 (regular) + 0 (collectable) = 48 balls.\n",
    "The answer is 48.\n",
    "\n",
    "Q: \"\"\"\n",
    "\n",
    "llm_call = f\"{context}\\n{one_shot_exemplar}{question}\\nA:\"\n",
    "_ = call_llm(model, parameters, llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfyjnV8Clxia"
   },
   "source": [
    "Next, increase `temperature` to .7 and use high `top_p` and `top_k` values to generate a different response.\n",
    "\n",
    "Run the next cell a few times and note how the answer changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 798
    },
    "id": "Fqr8DxNylcC1",
    "outputId": "b229260d-e5cc-448d-cf98-4f4933e12e73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc_parameters = {\n",
    "    \"temperature\": .7,\n",
    "    \"max_output_tokens\": 512,\n",
    "    \"top_p\": 1,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "\n",
    "_ = call_llm(model, sc_parameters, llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrbTQUGymnUr"
   },
   "source": [
    "As you rerun the code above, you'll see a variety of reasonings and answers.\n",
    "\n",
    "Next, loop and generate many responses, extract the answers, then output the answers from most to least common.\n",
    "\n",
    "This takes a few minutes to run. While it runs note the variety of reasonings and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5L1KRC6Hm5Ir",
    "outputId": "1ba863a2-dca1-46a8-88a1-bc31f3fc6b98",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter  # Easy counting of most common responses.\n",
    "sc_runs = 10\n",
    "responses = [None] * sc_runs\n",
    "answers = [None] * sc_runs\n",
    "\n",
    "for i in range(0, sc_runs):\n",
    "  print(f\"Response {i}...\")\n",
    "  responses[i] = call_llm(model,\n",
    "                          sc_parameters,\n",
    "                          llm_call,\n",
    "                          # Turn off printing LLM calls/responses.\n",
    "                          show_activity=False)\n",
    "  # If the response doesn't contain 'The answer is', the split fails.\n",
    "  # The split also fails if the answer contains a decimal or comma.\n",
    "  try:\n",
    "    answers[i] = responses[i].split(\"The answer is\")[1].split(\".\")[0].strip()\n",
    "  except Exception as e:\n",
    "    answers[i] = \"NA\"\n",
    "  print(responses[i])\n",
    "print(\"Answers and counts from most common to least common:\")\n",
    "print(Counter(answers).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxZ2S8hd9f33"
   },
   "source": [
    "The last output from the cell above is the counts of different answers. The correct answer (1475) should come back as the most common answer.\n",
    "\n",
    "The more LLM calls made, the greater the likelihood the most common answer is the correct answer.\n",
    "\n",
    "We can also plot the results to visualize the distribution of answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "OfJiXg_qWB0A",
    "outputId": "0ff21362-6f2d-4ae5-87d5-2ff400144d5b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thanks to Hans-Christian Fuchs for this.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(Counter(answers).keys(), Counter(answers).values())\n",
    "ax.tick_params(axis='x', rotation=55)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyMEmx1J_osN"
   },
   "source": [
    "### Self-Consistency Advantages\n",
    "\n",
    "1. Low-effort performance boost.\n",
    "1. Helps ideate chain-of-thought exemplars.\n",
    "1. Increased prompt robustness across different LLMs.\n",
    "1. Provides a pseudo \"confidence\" estimate based on the answer distributions.\n",
    "1. Opportunities to use \"average\" answers for problems without a single correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsaFThs-_pyG"
   },
   "source": [
    "### Self-Consistency Disadvantages\n",
    "\n",
    "1. Increased costs.\n",
    "1. Slower inference time and/or reduced throughput.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov281oL--eRh"
   },
   "source": [
    "### Self-Consistency Best Practices\n",
    "\n",
    "1. **Do** Use `temperature=.7`, `top_k=40`, `top_p=1`, and 10 responses as a starting point.\n",
    " * **Do** Experiment from there, different use cases may need different values.\n",
    " * **Do** Find optimal values for production use cases by conducting a hyperparameter search.\n",
    "   * Note that it's likely much more valuable to search on the response count than the LLM parameters, and if you do experiment with LLM parameters it's usually not worth reducing them much.\n",
    "1. **Do** Try self-consistency early if your initial prompt engineering attempts fail.\n",
    " * Self-consistency is more likely to boost performance than continuing to engineer your chain of thought prompt.  \n",
    "1. **Don't** Ignore cost and latency implications.\n",
    "1. **Do** Parallelize LLM calls to reduce execution time.\n",
    " * **Don't** Put off assessing the LLM throughput and latency your self-consistency use case requires.\n",
    "1. **Do** Use response distributions in creative ways. For example:\n",
    " * If fewer than X percent of answers match, flag the question for human review.\n",
    " * Generate multiple summaries and use a text similarity metric to identify which generated summary is most \"average\".\n",
    "1. **Do** Use self-consistency to inspire few-shot exemplars and to debug your prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhA8gbnohLo7"
   },
   "source": [
    "# Part 2: Actions, Retrieval, and Tool Use\n",
    "\n",
    "LLMs, like crows, are adept at using tools.\n",
    "\n",
    "<img src=\"./images/3-crow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD2iMx2wwBmN"
   },
   "source": [
    "## Hallucinations, Grounding, and Tools/Actions/Retrieval/RAG\n",
    "<a name=\"rag\"></a>\n",
    "\n",
    "LLMs are not reliable sources of facts. When an LLM response contains a correct fact, it is an emergent effect of what the LLM's parameters actually encode: probabilistic relationships between words.\n",
    "\n",
    "When factual accuracy is important, relying on these probabilistic relationships is risky.\n",
    "\n",
    "LLMs also cannot (yet) be retrained quickly or cheaply on the latest information. And even when retraining is possible catastrophic forgetting may lead to new errors in older information as the training dataset grows.\n",
    "\n",
    "When an LLM response is factually incorrect it is often called a \"hallucination\", though it's more accurately a [delusion](https://en.wikipedia.org/wiki/Delusion).\n",
    "\n",
    "Hallucinations can be missed by non-experts. LLM responses can be factually incorrect even while the generated text is grammatically accurate, well-formed, and confident in tone.\n",
    "\n",
    "See what output this LLM call gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "aD5WUUvDuHuD",
    "outputId": "40c6dc6a-bfd1-44fc-cbeb-f46e3a8da715",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Who is Chancellor of Germany?\"\n",
    "_ = call_llm(model, parameters, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VM5MjaAjm77V"
   },
   "source": [
    "The current model may respond correctly, but in August 2023, almost two years after Chancellor Merkel stepped down, this was the response:\n",
    "<img src=\"./images/6-hallucinate.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAENyzS3o2YO"
   },
   "source": [
    "The best way to manage hallucinations is to connect an LLM to an accurate and up-to-date external data source.\n",
    "\n",
    "\"Grounding\" is using external information to manage hallucinations. One way to \"ground\" is to insert external information into the LLM call, along with instructions to base the response on the inserted information.\n",
    "\n",
    "\"Retrieval Augmented Generation\" or \"RAG\" is a generic way of saying an LLM uses external knowledge. It can mean different things:\n",
    "1. An external retrieval system takes a user query as input then outputs information, which is then combined with the user query in the LLM call. (e.g., compare the embedding of the query to the embeddings of documents and insert the closest document into the LLM call). [Code sample](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-qa/question_answering_documents_langchain_matching_engine.ipynb).\n",
    "2. Call an LLM with instructions to formulate a retrieval call to an external information system based on a user's query, then make another call to the LLM combining the user's query and the retrieved information.\n",
    "3. Coupled bespoke retriever and generator deep learning models trained/tuned together (the focus of the original [RAG paper](https://arxiv.org/pdf/2005.11401.pdf)).\n",
    "\n",
    "This notebook focuses on #2, and uses the language \"tools\"/\"tool use\" to describe instructing an LLM to use an external system, avoiding the ambiguous term RAG. Later in part 3, we'll use \"actions\" and \"acting\" to match how ReAct is discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVj0W1lihch4"
   },
   "source": [
    "## How LLM Tool Use Works\n",
    "\n",
    "The basic pattern for LLM tool use is:\n",
    "1. Make a first LLM call describing:\n",
    " * i: The task you want completed.\n",
    " * ii: An external  system.\n",
    " * iii: How to formulate a call to the external system.\n",
    "2. Call the external system using the response generated by the LLM.\n",
    "3. Make a second LLM call that includes the response from the external system, along with instructions for the LLM to complete the original task using the response from the external system.\n",
    "\n",
    "If our LLM system is supposed to answer fact-based questions like the Chancellor example above:\n",
    "1. The first LLM call directs the LLM to generate a search query for a knowledge base.\n",
    "2. The LLM's response is used to query the knowledge base, and the result of the query is captured.\n",
    "3. The second LLM call includes the result of the knowledge base query, the original question, and instructions for the LLM to answer the question using the result from the knowledge base query.\n",
    "\n",
    "The LLM's tool can be many things--a database, a web search, a document retrieval system, etc. Part of the LLM system is the code integrating the LLM with the external information source.\n",
    "\n",
    "In this notebook, we'll use Wikipedia as an external information source and build a basic LLM system to answer fact-based questions. Our LLM system will:\n",
    "1. Call an LLM to generate a Wikipedia search query.\n",
    "1. Call the Wikipedia API to retrieve the query result.\n",
    "1. Call the LLM again with the Wikipedia API response plus the original question.\n",
    "\n",
    "Beyond the scope of this notebook, LLMs can be called with instructinos describing more than one tool. The LLM both selects the tool and formulates the call to the tool. And LLM tools don't have to be read-only, you can use a tool to interact with an external system (though please consider the ethics and fairness implications--just because you *can* use an LLM to automate an activity doesn't mean you *should*. Hallucinations are annoying when you want to do something like generate a summary, but can be devastating when making a decision that impacts someone's life. Even applications as seemingly innocent as automated paper grading can lead to model failures negatively impacting someone's life)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91FoLDpruqF4"
   },
   "source": [
    "## The Example Tool\n",
    "\n",
    "The function below takes a query, returns the top Wikipedia article match for the query, and then retrieves the first `return_chars` characters of the article.\n",
    "\n",
    "This tool is for teaching purposes and is somewhat limited. It cannot access lists or sidebars, does not handle suggestions well, does not support search within a Wikipedia article, and may not always return a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "2cLj2TiCt0cn",
    "outputId": "7a972a36-8fd2-499e-8962-85a30000fe6f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "def wiki_tool(query, return_chars = 1000):\n",
    "  try:\n",
    "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
    "  # If no exact match, take Wikipedia's auto-suggestion.\n",
    "  except wikipedia.exceptions.PageError as e:\n",
    "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
    "  snippet = page[0:return_chars]\n",
    "  return snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRZ6v1z0uWAd"
   },
   "source": [
    "Try the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "A4o-3Td9uZ-U",
    "outputId": "b9bdaec7-4d2a-401c-bf16-7661ceb327c0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_tool(\"chancellor of germany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7gGYXbxs8b7"
   },
   "source": [
    "## Chaining LLM Calls for Tool Use\n",
    "\n",
    "A basic two-step tool use LLM chain contains a few pieces, broken down here step-by-step.\n",
    "\n",
    "If you call the model (as of October 2023) with this example question about an obscure musician it hallucinates an incorrect answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "cHK1aJ_oXtJZ",
    "outputId": "66111769-f501-48fc-9160-c6911384310b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What musician released the album 'Somebody in the Snow'?\"\n",
    "_ = call_llm(model, parameters, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nqc0vhU5H1X"
   },
   "source": [
    "### Step 1: Provide the LLM Instructions for Using the Tool\n",
    "\n",
    "You must provide the LLM both instructions for your task and for how to use the tool.\n",
    "\n",
    "This \"instructions\" part of the LLM call is sometimes called the \"context\" or some variation of \"condition\" (\"conditioning\", \"conditioning prompt\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "rhWpoRFGA21n",
    "outputId": "07d6e495-39ea-493a-bfa1-5e8559f7d89b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Answer questions using a lookup of Wikipedia.\n",
    "After each question, write a Wikipedia search followed by '<STOP>'.\n",
    "Do not include extra adjectives or descriptors in the search string.\n",
    "The Wikipedia search will be used to retrieve the most relevant content.\n",
    "A section of the Wikipedia article will then be sent to the next LLM call.\n",
    "Use the text of the Wikipedia article to answer the question.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqWY6f3EBDyO"
   },
   "source": [
    "### Step 2: Provide An Exemplar\n",
    "\n",
    "The LLM needs exemplars that show how to use the tool to complete the task.\n",
    "\n",
    "This example has only a one-shot exemplar, few-shot would be better.\n",
    "\n",
    "The Wikipedia article text in this exemplar comes from running `wiki_tool(\"chancellor of germany\")` in August 2023.\n",
    "\n",
    "Note: After future retrainings the LLM will answer this question correctly without an external tool. But this one-shot exemplar will still work, since it shows the pattern of a Wikipedia search, a response, and an answer based on the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Haoj8nWSA_fy",
    "outputId": "822063b0-81f0-4796-d2fa-89f566a49293",
    "tags": []
   },
   "outputs": [],
   "source": [
    "exemplar = \"\"\"Question: Who is Chancellor of Germany?\n",
    "Wikipedia Search: chancellor of Germany<STOP>\n",
    "Wikipedia Article: The chancellor of Germany, officially the federal chancellor of the Federal Republic of Germany, is the head of the federal government of Germany, and the commander in chief of the German Armed Forces during wartime. The chancellor is the chief executive of the Federal Cabinet and heads the executive branch. The chancellor is elected by the Bundestag on the proposal of the federal president and without debate (Article 63 of the German Constitution).The current officeholder is Olaf Scholz of the SPD, who was elected in December 2021, succeeding Angela Merkel. He was elected after the SPD entered into a coalition agreement with Alliance 90/The Greens and the FDP.\\n\\n\\n== History of the office ==\\nThe office of Chancellor has a long history, stemming back to the Holy Roman Empire, when the office of German archchancellor was usually held by archbishops of Mainz. The title was, at times, used in several states of German-speaking Europe. The modern office of chancellor was established with the\n",
    "Answer: Olaf Scholz\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deMaQ9ddDQhc"
   },
   "source": [
    "### Step 3: Make the First Call in the LLM Chain\n",
    "\n",
    "We'll combine our context and our exemplar together with our question and make a call to the LLM asking for a Wikipedia search query as a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PC4l5oHtD9OO",
    "outputId": "7da83524-a7ee-4374-b7ad-a41a60ad282e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_one_call = f\"\"\"{context}\n",
    "\n",
    "{exemplar}\n",
    "\n",
    "Question: {question}\n",
    "Wikipedia Search:\"\"\"\n",
    "step_one_response = call_llm(model, parameters, step_one_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDUi5JB8GCL9"
   },
   "source": [
    "### Step 4: Use the LLM's Response to Query the Tool\n",
    "\n",
    "Note the LLM response contains more than the Wikipedia search query.\n",
    "\n",
    "LLMs work by repeatedly predicting the next token over and over again, based on the tokens in the LLM call plus any previously predicted tokens. This means the LLM will generate excess text, it does not know to stop after the Wikipedia search query.\n",
    "\n",
    "Everything beyond the Wikipedia search query is garbage. The excess text is discarded using the `<STOP>` signifier, though this could also be done with line breaks.\n",
    "\n",
    "In a production system, it's important to control costs by limiting the response size when making an LLM call like this.\n",
    "\n",
    "The following function takes the LLM response from the first chain step and returns the Wikipedia query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "_2cqh5R4HTHV",
    "outputId": "171799dc-f330-4389-8aa9-18fedf17089c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wiki_query (llm_response, stop_text = \"<STOP>\"):\n",
    "  # Assumes the query is in the first line.\n",
    "  first_line = llm_response.splitlines()[0]\n",
    "  query = first_line.split(stop_text)[0]\n",
    "  return query.strip() # Remove leading and trailing whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sv6ox89JYPe"
   },
   "source": [
    "Use this function on the response from the previous LLM call to extract the query, then  use `wiki_tool` to search Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "0d5CKJRyJW5C",
    "outputId": "03d81998-97cd-4332-dfb0-c2666b4f188a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_query = get_wiki_query(step_one_response)\n",
    "print(f\"Tool Query: {wiki_query}\")\n",
    "wiki_text = wiki_tool(wiki_query)\n",
    "print(f\"Wikipedia Snippet: {wiki_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAmH5sQddF9Q"
   },
   "source": [
    "### Step 5: Use the Tool Response to Make the Second Call in the LLM Chain\n",
    "\n",
    "Next, answer the question by taking the output from the tool and constructing a second LLM call.\n",
    "\n",
    "LLM tool usage generally maintains the history of the previous calls and responses. To construct the second call in the chain:\n",
    "1. Start with the first LLM call in the chain.\n",
    "1. Append the previously generated Wikipedia query.\n",
    "1. Append the Wikipedia search result.\n",
    "\n",
    "Here's a reminder of what our first call looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "UJKx_TKAdmRz",
    "outputId": "97b41a40-de62-42f1-fa8c-014f27faf891",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(step_one_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCCGiIZuChoA"
   },
   "source": [
    "This first LLM call is combined with the query from the first LLM response and the output from the Wikipedia tool, along with structure to match the exemplar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "gRsLkHfRd3hY",
    "outputId": "41b76607-4dcd-4455-a3cb-b9e215e74c9d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_two_call = f\"\"\"{step_one_call} {wiki_query}\n",
    "Wikipedia Article: {wiki_text}\n",
    "Answer: \"\"\"\n",
    "step_two_response = call_llm(model, parameters, step_two_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pU-UI3mnKPLq"
   },
   "source": [
    "## Putting All the Steps Together\n",
    "\n",
    "This code snippet below gathers all the steps above, dependent packages, and dependent functions into a single function that manages the two-step tool usage LLM chain.\n",
    "\n",
    "You can copy and paste this code into your own project and it should work, assuming you've installed the right packages and authenticated. Do not forget to update the `PROJECT_ID` and `LOCATION` variables if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "o__JbR9LKiNX",
    "outputId": "a13b0ff0-b8c2-4f9a-d0af-24adb61e2fcb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import wikipedia\n",
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "PROJECT_ID = \"YOUR-PROJECT-ID\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "# Code examples may misbehave if the model is changed.\n",
    "MODEL_NAME = \"gemini-1.5-pro\"\n",
    "\n",
    "# Set up Vertex AI.\n",
    "\n",
    "\n",
    "vertexai.init(project=PROJECT_ID,\n",
    "              location=LOCATION)\n",
    "parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "\n",
    "model = GenerativeModel(MODEL_NAME)\n",
    "\n",
    "def call_llm(model, parameters, llm_call, show_activity=True, max_retries=5):\n",
    "    \n",
    "    generation_config = GenerationConfig(**parameters)\n",
    "    \n",
    "    attempt = 1\n",
    "    \n",
    "    while attempt <= max_retries:\n",
    "    \n",
    "        try:\n",
    "            output = model.generate_content(llm_call, generation_config=generation_config)\n",
    "            response = str(output.candidates[0].content.parts[0]).split(\"text:\")[1]\n",
    "\n",
    "            if show_activity:\n",
    "                BOLD = \"\\033[1m\"\n",
    "                UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
    "                print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
    "                print(f\"{BOLD}The response:{UNFORMAT}\\n{response}\")\n",
    "\n",
    "            return response  # Return to `_` if not needed.\n",
    "\n",
    "        except ResourceExhausted as e:\n",
    "            time.sleep(2**attempt)\n",
    "            attempt += 1\n",
    "            if attempt > max_retries:\n",
    "                raise ResourseExhausted(e)\n",
    "            continue\n",
    "\n",
    "\n",
    "def wiki_tool(query, return_chars = 1000):\n",
    "  try:\n",
    "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
    "  # If no exact match, take Wikipedia's suggestion.\n",
    "  except wikipedia.exceptions.PageError as e:\n",
    "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
    "  snippet = page[0:return_chars]\n",
    "  return snippet\n",
    "\n",
    "\n",
    "def get_wiki_query (llm_response, stop_text = \"<STOP>\"):\n",
    "  # Extract the wikipedia query from the LLM response.\n",
    "  # Assumes the query is in the first line.\n",
    "  first_line = llm_response.splitlines()[0]\n",
    "  query = first_line.split(stop_text)[0]\n",
    "  return query.strip() # Remove leading and trailing whitespace\n",
    "\n",
    "\n",
    "def wiki_tool_chain(model,\n",
    "                    parameters,\n",
    "                    context,\n",
    "                    exemplar,\n",
    "                    question,\n",
    "                    show_activity=False):\n",
    "  # Answer a query using wikipedia by calling an LLM.\n",
    "  step_one_call = (\n",
    "      f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nWikipedia Search:\"\n",
    "  )\n",
    "  if show_activity:\n",
    "    print(\"\\033[1mMaking the first LLM call...\\033[0m\\x1B[0m\")\n",
    "  step_one_response = call_llm(model, parameters, step_one_call, show_activity)\n",
    "  wiki_query = get_wiki_query(step_one_response)\n",
    "  wiki_text = wiki_tool(wiki_query)\n",
    "\n",
    "  step_two_call = (\n",
    "      f\"{step_one_call} {wiki_query}\\nWikipedia Article: {wiki_text}\\nAnswer: \"\n",
    "  )\n",
    "  if show_activity:\n",
    "    print(\"\\033[1mMaking the second LLM call...\\033[0m\\x1B[0m\")\n",
    "  step_two_response = call_llm(model, parameters, step_two_call, show_activity)\n",
    "\n",
    "  return step_two_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oukbFAyxoNPR"
   },
   "source": [
    "With `show_activity = True` to see the breakdown of the LLM calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uU3h3GkcbgUn",
    "outputId": "2959234c-d6a6-4fec-ea4e-6fae613eb4f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_tool_chain(model,\n",
    "                parameters,\n",
    "                context,\n",
    "                exemplar,\n",
    "                \"What musician released the album 'Somebody in the Snow'?\",\n",
    "                show_activity = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjKjBgbMXWyZ"
   },
   "source": [
    "Try experimenting with changing the `question`. Keep `show_activity = True` to see the two steps in the LLM chain.\n",
    "\n",
    "This doesn't work well with many questions. As mentioned above, our tool is not very good, and it will fail entirely on some questions.\n",
    "\n",
    "Tool use best practices are [discussed more in part 3](#react-tools).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH4Z5cqnmkpM"
   },
   "source": [
    "# Part 3: ReAct (Reasoning + Acting) Prompting\n",
    "\n",
    "ReAct (reasoning + actions) combines chain of thought and tool usage together to reason through complex tasks by interacting with external systems.\n",
    "\n",
    "ReAct-style prompting is currently (Fall 2023) the state-of-the-art for most prompt-driven LLM tasks. When you use plugins or extensions, where an LLM or LLM-based chatbot or system interacts with an external system, you are using a ReAct-style system. In general, any LLM system that reflects up-to-date knowledge is invisibly using ReAct-style functionality under-the-hood.\n",
    "\n",
    "An LLM attempting to interact with an external system:\n",
    "\n",
    "<img src=\"./images/4-robot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mepNl0JxmsTF"
   },
   "source": [
    "## ReAct Basics\n",
    "\n",
    "ReAct chains typically have three interleaved parts:\n",
    "- **Thoughts**: Like in chain of thought, these are waypoints, plans, reasoning, etc. generated by the LLM as it makes progress towards the final output.\n",
    "- **Actions**: LLM-generated commands, calls, or instructions to access an external system. The external system may be a tool that provides information, but can also be more general (i.e., the action observes or changes the state of an external system).\n",
    "- **Observations**: A response, feedback, result, etc. from the external system, inserted into an LLM call to generate the next thought.\n",
    "\n",
    "These three steps are repeated until the LLM completes its task.\n",
    "\n",
    "Similar to chain-of-thought prompting, this repeated cycle forms an \"internal monologue\" or \"inner speech\", but with the important addition of decisions to act and feedback from the actions beyond just the reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cZ-EbgBm5Zz"
   },
   "source": [
    "### What a ReAct Chain Looks Like\n",
    "\n",
    "Before breaking down the LLM calls in a ReAct chain, it helps to see what a complete ReAct chain looks like.\n",
    "\n",
    "The actions in this chain are Wikipedia lookups, and the observations are snippets from the Wikipedia article.\n",
    "\n",
    "The original call to the LLM is:\n",
    "```Question: Who was born first, Ronald Regan or Gerald Ford?```(ignoring instructions, exemplars, etc. for now).\n",
    "\n",
    "The completed ReAct chain looks like this. Scroll to the right to read the full observations:\n",
    "\n",
    "```\n",
    "Question: Who was born first, Ronald Reagan or Gerald Ford?\n",
    "Thought 1: I need to look up Ronald Reagan and see when he was born.\n",
    "Action 1: Ronald Reagan<STOP>\n",
    "Observation 1: Ronald Wilson Reagan (February 6, 1911 – June 5, 2004) was an American politician and actor who served as the 40th president of the United States from 1981 to 1989. A conservative, he was the first president from the West Coast and the first divorced president. Reagan was born in Tampico, Illinois, and raised in Dixon, Illinois. He was educated at Eureka College, where he studied economics and sociology. After graduating, Reagan moved to California, where he became a radio sports announcer. He later moved into acting, appearing in over 50 films. Reagan served as president of the Screen Actors Guild from 1947 to 1952.\n",
    "Thought 2: Ronald Reagan was born in 1911. I need to look up Gerald Ford and see when he was born.\n",
    "Action 2: Gerald Ford<STOP>\n",
    "Observation 2: Gerald Rudolph Ford Jr. ( JERR-əld; born Leslie Lynch King Jr.; July 14, 1913 – December 26, 2006) was an American politician who served as the 38th president of the United States from 1974 to 1977. He previously served as the leader of the Republican Party in the U.S. House of Representatives from 1965 to 1973, when he was appointed the 40th vice president by President Richard Nixon, after Spiro Agnew's resignation. Ford succeeded to the presidency when Nixon resigned in 1974, but was defeated for election to a full term in 1976. Ford is the only person to become U.S. president without winning an election for president or vice president. Ford was born in Omaha, Nebraska and raised in Grand Rapids, Michigan. He attended the University of Michigan, where he played for the school's football team before eventually attending Yale Law School. Afterward, he served in the U.S. Naval Reserve from 1942 to 1946. Ford began his political career in 1949 as the U.S. representative from Michigan's 5\n",
    "Thought 3: Gerald Ford was born in 1913. 1911 is before 1913. Answer[Ronald Reagan]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlDmYHuInLVb"
   },
   "source": [
    "### Breaking Down a ReAct Chain\n",
    "\n",
    "The example ReAct chain above is constructed from three LLM calls.\n",
    "\n",
    "Note the responses in this section have been stripped of extra predicted text, similar to how extra text was stripped in the part 2 tool use discussion.\n",
    "\n",
    "**Call 1:**\n",
    "```\n",
    "Question: Who was born first, Ronald Reagan or Gerald Ford?\n",
    "Thought 1:\n",
    "```\n",
    "**Response 1:**\n",
    "```\n",
    "I need to look up Ronald Reagan and see when he was born.\n",
    "Action 1: Ronald Reagan<STOP>\n",
    "```\n",
    "\n",
    "Each LLM call after the first is:\n",
    "1. The previous LLM call plus\n",
    "1. The LLM response to the previous call plus\n",
    "1. The wikipedia lookup result plus\n",
    "1. \"Thought #:\"\n",
    "\n",
    "**Call 2:**\n",
    "\n",
    "Call 2 is created by concatenating call 1 + response 1 + the result of the wikipedia lookup (in the observation) + \"Thought 2:\".\n",
    "```\n",
    "Question: Who was born first, Ronald Reagan or Gerald Ford?\n",
    "Thought 1: I need to look up Ronald Reagan and see when he was born.\n",
    "Action 1: Ronald Reagan<STOP>\n",
    "Observation 1: Ronald Wilson Reagan (February 6, 1911 – June 5, 2004) was an American politician and actor who served as the 40th president of the United States from 1981 to 1989. A conservative, he was the first president from the West Coast and the first divorced president. Reagan was born in Tampico, Illinois, and raised in Dixon, Illinois. He was educated at Eureka College, where he studied economics and sociology. After graduating, Reagan moved to California, where he became a radio sports announcer. He later moved into acting, appearing in over 50 films. Reagan served as president of the Screen Actors Guild from 1947 to 1952.\n",
    "Thought 2:\n",
    "```\n",
    "\n",
    "**Response 2:**\n",
    "```\n",
    "Ronald Reagan was born in 1911. I need to look up Gerald Ford and see when he was born.\n",
    "Action 2: Gerald Ford<STOP>\n",
    "```\n",
    "\n",
    "**Call 3:**\n",
    "\n",
    "Just like in call 2, we create call 3 by concatenating call 2 + response 2 + the result of the wikipedia lookup + \"Thought 3:\".\n",
    "\n",
    "```\n",
    "Question: Who was born first, Ronald Reagan or Gerald Ford?\n",
    "Thought 1: I need to look up Ronald Reagan and see when he was born.\n",
    "Action 1: Ronald Reagan<STOP>\n",
    "Observation 1: Ronald Wilson Reagan (February 6, 1911 – June 5, 2004) was an American politician and actor who served as the 40th president of the United States from 1981 to 1989. A conservative, he was the first president from the West Coast and the first divorced president. Reagan was born in Tampico, Illinois, and raised in Dixon, Illinois. He was educated at Eureka College, where he studied economics and sociology. After graduating, Reagan moved to California, where he became a radio sports announcer. He later moved into acting, appearing in over 50 films. Reagan served as president of the Screen Actors Guild from 1947 to 1952.\n",
    "Thought 2: Ronald Reagan was born in 1911. I need to look up Gerald Ford and see when he was born.\n",
    "Action 2: Gerald Ford<STOP>\n",
    "Observation 2: Gerald Rudolph Ford Jr. ( JERR-əld; born Leslie Lynch King Jr.; July 14, 1913 – December 26, 2006) was an American politician who served as the 38th president of the United States from 1974 to 1977. He previously served as the leader of the Republican Party in the U.S. House of Representatives from 1965 to 1973, when he was appointed the 40th vice president by President Richard Nixon, after Spiro Agnew's resignation. Ford succeeded to the presidency when Nixon resigned in 1974, but was defeated for election to a full term in 1976. Ford is the only person to become U.S. president without winning an election for president or vice president.\n",
    "Ford was born in Omaha, Nebraska and raised in Grand Rapids, Michigan. He attended the University of Michigan, where he played for the school's football team before eventually attending Yale Law School. Afterward, he served in the U.S. Naval Reserve from 1942 to 1946. Ford began his political career in 1949 as the U.S. representative from Michigan's 5\n",
    "Thought 3:\n",
    "```\n",
    "\n",
    "Finally, the LLM returns an answer.\n",
    "\n",
    "**Response 3:**\n",
    "```\n",
    "Gerald Ford was born in 1913. 1911 is before 1913. Answer[Ronald Reagan]\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX060zm2p4A5"
   },
   "source": [
    "## Manually Running a ReAct Chain\n",
    "\n",
    "This section runs a ReAct chain step-by-step.\n",
    "\n",
    "A few things are required, all in the next code cell:\n",
    "1. Instructions (context) for the LLM to understand how to do ReAct.\n",
    "2. At least one exemplar.\n",
    "3. A tool to execute the LLM's actions.\n",
    "4. A Vertex AI language model to make LLM calls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "xk1oTh8HuXoB",
    "outputId": "f1c94b50-c808-42ab-895c-047584bcb3d6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Answer questions with thoughts, actions, and observations.\n",
    "\n",
    "Think about the next action to take. Then take an action.\n",
    "All actions are a lookup of Wikipedia.\n",
    "The Wikipedia action returns the beginning of the best-matching article.\n",
    "When making a Wikipedia lookup action, end the lookup with <STOP>.\n",
    "After the Wikipedia action, you will have an observation.\n",
    "The observation is based on what you learn from the Wikipedia lookup action.\n",
    "After the observation, begin the loop again with a thought.\n",
    "\n",
    "Repeat as necessary a thought, taking an action, and having an observation.\n",
    "Keep repeating as necessary until you know the answer to the question.\n",
    "When you think you have an answer, return the answer in the format:\n",
    "\"Answer[answer goes here between square brackets]\" as part of a thought.\n",
    "Make sure to capitalize \"Answer\".\n",
    "\n",
    "Only use information in the observations to answer the question.\"\"\"\n",
    "\n",
    "exemplar = \"\"\"Example:\n",
    "Question: Who was born first, Ronald Reagan or Gerald Ford?\n",
    "Thought 1: I need to look up Ronald Reagan and see when he was born.\n",
    "Action 1: Ronald Reagan<STOP>\n",
    "Observation 1: Ronald Wilson Reagan (February 6, 1911 – June 5, 2004) was an American politician and actor who served as the 40th president of the United States from 1981 to 1989. A conservative, he was the first president from the West Coast and the first divorced president. Reagan was born in Tampico, Illinois, and raised in Dixon, Illinois. He was educated at Eureka College, where he studied economics and sociology. After graduating, Reagan moved to California, where he became a radio sports announcer. He later moved into acting, appearing in over 50 films. Reagan served as president of the Screen Actors Guild from 1947 to 1952.\n",
    "Thought 2: Ronald Reagan was born in 1911. I need to look up Gerald Ford and see when he was born.\n",
    "Action 2: Gerald Ford<STOP>\n",
    "Observation 2: Gerald Rudolph Ford Jr. ( JERR-əld; born Leslie Lynch King Jr.; July 14, 1913 – December 26, 2006) was an American politician who served as the 38th president of the United States from 1974 to 1977. He previously served as the leader of the Republican Party in the U.S. House of Representatives from 1965 to 1973, when he was appointed the 40th vice president by President Richard Nixon, after Spiro Agnew's resignation. Ford succeeded to the presidency when Nixon resigned in 1974, but was defeated for election to a full term in 1976. Ford is the only person to become U.S. president without winning an election for president or vice president.\n",
    "Ford was born in Omaha, Nebraska and raised in Grand Rapids, Michigan. He attended the University of Michigan, where he played for the school's football team before eventually attending Yale Law School. Afterward, he served in the U.S. Naval Reserve from 1942 to 1946. Ford began his political career in 1949 as the U.S. representative from Michigan's 5\n",
    "Thought 3: Gerald Ford was born in 1913. 1911 is before 1913. Answer[Ronald Reagan]\"\"\"\n",
    "\n",
    "# Code for calling Wikipedia.\n",
    "import wikipedia\n",
    "def wiki_tool(query, return_chars = 1000):\n",
    "  try:\n",
    "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
    "  # If no exact match, take Wikipedia's suggestion.\n",
    "  except wikipedia.exceptions.PageError as e:\n",
    "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
    "  snippet = page[0:return_chars]\n",
    "  return snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5wOlmfAv53K"
   },
   "source": [
    "The first LLM call is the context, the exemplar, the question, and a label for the first thought.\n",
    "\n",
    "The action/thought/observation labels at the start of each line are important to ReAct  chains, and increase the likelihood the LLM response sticks to the \"script\" of interleaved ReAct steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "afRXzBhlwBw6",
    "outputId": "bb4546e1-6be3-467e-b30b-8245235d7bcf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"When was the opening year of the theater that debuted Ibsen's 'A Doll's House'?\"\n",
    "llm_call_1 = f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nThought 1:\"\n",
    "print(llm_call_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vFm05Ymwwjwc",
    "outputId": "c12064f1-5583-44b8-d87f-535ee3c3aa53",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_1 = call_llm(model, parameters, llm_call_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVPLsqolw16U"
   },
   "source": [
    "The first and second lines of the response are good. The model generated a reasonable thought and appropriate action.\n",
    "\n",
    "But just as in the tool use section above, the LLM continues generating garbage text. Remember, LLMs repeatedly predict the next token, and in a ReAct-style LLM call those next tokens are the LLM's prediction of the rest of the ReAct chain.\n",
    "\n",
    "Just like in the tool use section, extra text is discarded. Only the first two response lines are kept:`Thought 1` and `Action 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "UbgFW4Ehy6gh",
    "outputId": "228366dc-2b3c-423a-90e5-9a9675ca9ed5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only take the first two lines of the response.\n",
    "# Splitlines returns a list with an item for each line.\n",
    "response_1 = response_1.splitlines()[0:2]\n",
    "# Turn response 1 into text from the list so we can concatenate to llm call 1.\n",
    "response_1 = (\"\\n\").join(response_1)\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZxIR6nmuCLl"
   },
   "source": [
    "Next, query the Wikipedia tool with the LLM's `Action 1` response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "wU7ExxFq0odj",
    "outputId": "e08bf3df-0f98-42dc-d4aa-40be6bcc28a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look up the LLM's action in Wikipedia.\n",
    "wiki_text_1 = wiki_tool(\"A Doll's House\")\n",
    "print(wiki_text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAWEAjWw3MyU"
   },
   "source": [
    "Then construct the next LLM call by adding the Wikipedia tool output as `Observation 1` and then appending `Thought 2:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "nn4C9X7H0vT4",
    "outputId": "8ad172f7-4ca0-4505-8ace-b49a6f4bccb4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct the next LLM call.\n",
    "llm_call_2 = f\"{llm_call_1} {response_1}\\nObservation 1: {wiki_text_1}\\nThought 2:\"\n",
    "print(llm_call_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kNS0yZre1Obb",
    "outputId": "0c272587-b85e-4a04-bfd0-80399fe6f1a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_2 = call_llm(model, parameters, llm_call_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dJZdWI91Xut"
   },
   "source": [
    "For the third LLM call in the ReAct chain follow the same procedure as the second call:\n",
    "1. Take the first two lines of the response.\n",
    "2. Look up the action in Wikipedia.\n",
    "3. Assemble the LLM call from the response, the Wikipedia output, and the previous LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "yioUsUmI1mdf",
    "outputId": "a50ce940-091b-4b63-eb57-07cb4c3724e2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only take the first two lines of the response.\n",
    "# Splitlines returns a list with an item for each line.\n",
    "response_2 = response_2.splitlines()[0:2]\n",
    "# Turn response 1 into text from the list so we can concatenate to llm call 1.\n",
    "response_2 = (\"\\n\").join(response_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "plRMm1DS1mdf",
    "outputId": "a0b26513-a431-4f41-f737-8c5d86c5021c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look up the LLM's action in Wikipedia.\n",
    "wiki_text_2 = wiki_tool(\"Royal Theatre in Copenhagen, Denmark\")\n",
    "print(wiki_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JEqsooGh1mdf",
    "outputId": "e035e9aa-4505-4f64-ef35-a0f2413688d7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct the next LLM call.\n",
    "llm_call_3 = f\"{llm_call_2} {response_2}\\nObservation 2: {wiki_text_2}\\nThought 3:\"\n",
    "print(llm_call_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-gWWK89A1mdf",
    "outputId": "787271fb-d743-4957-bad1-081fcc6140b0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_3 = call_llm(model, parameters, llm_call_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsVNvZ5F2HjV"
   },
   "source": [
    "And we have an answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfmXEYdg2aMb"
   },
   "source": [
    "## A Complete Python Code Snippet for Running ReAct Chains\n",
    "\n",
    "To use ReAct in an application you need to automate the previous manually-executed steps.\n",
    "\n",
    "The instructive code snippet below runs ReAct chains. It makes formatted ReAct calls to the LLM, extracts actions, executes actions, detects if the LLM has responded with an answer, and loops.\n",
    "\n",
    "It's **highly** recommended you walk through the code below and read the comments to better understand how the ReAct chain is automated.\n",
    "\n",
    "This isn't production-ready code:\n",
    "1. The snippet is hardcoded to this specific and minimal ReAct example. ReAct chains can look different (more on this later), and useful applications built with ReAct chains  require customized tools.\n",
    "2. The snippet is brittle, especially the bare-bones Wikipedia tool.\n",
    "3. The LLM may re-predict previous actions, causing ReAct to infinitely loop. This snippet stops after `max_steps` LLM calls, production ReAct code should catch the loop and attempt to recover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "PVc3xRoWw1HM",
    "outputId": "07f058e8-07b6-4fb6-bfa5-d2ae113d6f72",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import wikipedia\n",
    "import vertexai\n",
    "\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "PROJECT_ID = \"YOUR-PROJECT-ID\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "# Code examples may misbehave if the model is changed.\n",
    "MODEL_NAME = \"gemini-1.5-pro\"\n",
    "\n",
    "# Set up Vertex AI.\n",
    "\n",
    "\n",
    "vertexai.init(project=PROJECT_ID,\n",
    "              location=LOCATION)\n",
    "parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "\n",
    "model = GenerativeModel(MODEL_NAME)\n",
    "\n",
    "def call_llm(model, parameters, llm_call, show_activity=True, max_retries=5):\n",
    "    \n",
    "    generation_config = GenerationConfig(**parameters)\n",
    "    \n",
    "    attempt = 1\n",
    "    \n",
    "    while attempt <= max_retries:\n",
    "    \n",
    "        try:\n",
    "            output = model.generate_content(llm_call, generation_config=generation_config)\n",
    "            response = str(output.candidates[0].content.parts[0]).split(\"text:\")[1]\n",
    "\n",
    "            if show_activity:\n",
    "                BOLD = \"\\033[1m\"\n",
    "                UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
    "                print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
    "                print(f\"{BOLD}The response:{UNFORMAT}\\n{response}\")\n",
    "\n",
    "            return response  # Return to `_` if not needed.\n",
    "\n",
    "        except ResourceExhausted as e:\n",
    "            time.sleep(2**attempt)\n",
    "            attempt += 1\n",
    "            if attempt > max_retries:\n",
    "                raise ResourseExhausted(e)\n",
    "\n",
    "                \n",
    "def wiki_tool(query, return_chars = 1000):\n",
    "  try:\n",
    "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
    "  # If no exact match, take Wikipedia's suggestion.\n",
    "  except wikipedia.exceptions.PageError as e:\n",
    "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
    "  snippet = page[0:return_chars]\n",
    "  return snippet\n",
    "\n",
    "\n",
    "def wiki_react_chain(model,\n",
    "                     parameters,\n",
    "                     context,\n",
    "                     exemplar,\n",
    "                     question,\n",
    "                     max_steps=7,\n",
    "                     show_activity=False):\n",
    "  # Call an LLM in a ReACT-style Thought -> Action -> Observation loop.\n",
    "  # Call the LLM max_steps times or to an answer in the pattern Answer[ans].\n",
    "\n",
    "  # Construct the first LLM call, teeing up the first thought.\n",
    "  next_llm_call = f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nThought 1:\"\n",
    "\n",
    "  step = 1\n",
    "  while step <= max_steps:\n",
    "\n",
    "    if show_activity:\n",
    "      print(f\"\\033[1mReAct chain step {step}:\\033[0m\\x1B[0m\")\n",
    "    llm_response = call_llm(model, parameters, next_llm_call, show_activity)\n",
    "\n",
    "    # Check for an answer. Look only at the first line of the response, since\n",
    "    #   the LLM will continue predicting beyond the next thought.\n",
    "    # This is brittle, it assumes no line breaks in the thought.\n",
    "    response_first_line = llm_response.splitlines()[0]\n",
    "    first_line_answer_split = response_first_line.split(\"Answer[\")\n",
    "    if len(first_line_answer_split) > 1:  # If there's a split on \"Answer[\".\n",
    "      # Return the answer, removing the \"]\" that comes after the answer.\n",
    "      return first_line_answer_split[1].split(\"]\")[0]\n",
    "\n",
    "    # If no answer, assume following response line is action.\n",
    "    response_second_line = llm_response.splitlines()[1]\n",
    "    \"\"\"\n",
    "      Note the hard coded \"<STOP>\" characters marking the end of the action.\n",
    "      This isn't strictly necessary if we assume the first line in the LLM\n",
    "      response is the thought and the second is the action, and that any\n",
    "      subsequent lines are garbage. But instructing the LLM to explicitly signal\n",
    "      structure it the response often gives more structurally consistent\n",
    "      responses, and also makes it easier to detect one way ReAct can fail.\n",
    "    \"\"\"\n",
    "    # Extract the wiki query from the action line of the response.\n",
    "    wiki_query = response_second_line.split(\":\")[1].split(\"<STOP>\")[0]\n",
    "    # Remove leading/trailing whitespace.\n",
    "    wiki_query = wiki_query.strip()\n",
    "    if show_activity:\n",
    "      print(f\"\\033[1mQuerying wikipedia for: {wiki_query}.\\033[0m\\x1B[0m\")\n",
    "    wiki_text = wiki_tool(wiki_query)\n",
    "\n",
    "    # Assemble the next LLM call.\n",
    "    # Only use the lines of the LLM response with the first thought and action.\n",
    "    usable_response = f\"{response_first_line}\\n{response_second_line}\"\n",
    "    # Assemble the wiki response into the observation line.\n",
    "    obs = f\"Observation {step}: {wiki_text}\"\n",
    "    step += 1\n",
    "    # Previous llm call + the first action and thought in the response +\n",
    "    # the result of the wikipedia lookup = llm call for next ReAct step.\n",
    "    # Note that next_llm_call was the last call we made, but we reassign it to\n",
    "    #   the same variable name so the loop works.\n",
    "    next_llm_call = f\"{next_llm_call} {usable_response}\\n{obs}\\nThought {step}:\"\n",
    "\n",
    "  # If max_steps exceeded and the loop exits.\n",
    "  # Would be better to raise an exception.\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7L113wd8gdy"
   },
   "source": [
    "Experiment with changing the `question` above. You may not get great results. This might be due to the brittle Wikipedia tool, but you may see errors with ReAct as well.\n",
    "\n",
    "Think about how you could improve the performance of the ReAct failures by changing the context or exemplars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joeGGFHunvFW"
   },
   "source": [
    "## More ReAct Use Cases\n",
    "\n",
    "The ReAct pattern does more than answer questions.\n",
    "\n",
    "With a different context and exemplar, the ReAct code snippet above is adapted for fact checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qD06UTNDoVIm",
    "outputId": "772d2c05-34dc-4746-ad96-efb9e16d69c0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"The GDP of Japan is higher than the GDP of BRICS.\"\n",
    "\n",
    "context = \"\"\"You are verifying claims as true or false.\n",
    "Verify the claim with thoughts, actions, and observations.\n",
    "Determine if there is an observation that SUPPORTS or REFUTES the claim.\n",
    "\n",
    "Think about the next action to take to verify the claim. Then take an action.\n",
    "All actions are a lookup of wikipedia.\n",
    "The wikipedia action returns the beginning of the best-matching article.\n",
    "When making a wikipedia lookup action, end the lookup with <STOP>.\n",
    "After the wikipedia action, you will make an observation.\n",
    "The observation is based on what you learn from the wikipedia lookup action.\n",
    "After the observation, begin the loop again with a thought.\n",
    "\n",
    "Repeat as necessary a thought, taking an action, and making an observation.\n",
    "Keep repeating as necessary until you reach a conclusion about the claim.\n",
    "If an observation refutes the claim, return the answer as \"Answer[REFUTES]\".\n",
    "If an observation supports the claim, return the answer as \"Answer[SUPPORTS]\".\n",
    "\n",
    "Only use information in the observations to answer the question.\"\"\"\n",
    "\n",
    "exemplar = \"\"\"Example:\n",
    "Claim: Ronald Reagan was born before Gerald Ford.\n",
    "Thought 1: I need to look up Ronald Reagan and see when he was born.\n",
    "Action 1: Ronald Reagan<STOP>\n",
    "Observation 1: Ronald Wilson Reagan (February 6, 1911 – June 5, 2004) was an American politician and actor who served as the 40th president of the United States from 1981 to 1989. A conservative, he was the first president from the West Coast and the first divorced president. Reagan was born in Tampico, Illinois, and raised in Dixon, Illinois. He was educated at Eureka College, where he studied economics and sociology. After graduating, Reagan moved to California, where he became a radio sports announcer. He later moved into acting, appearing in over 50 films. Reagan served as president of the Screen Actors Guild from 1947 to 1952.\n",
    "Thought 2: Ronald Reagan was born in 1911. I need to look up Gerald Ford and see when he was born.\n",
    "Action 2: Gerald Ford<STOP>\n",
    "Observation 2: Gerald Rudolph Ford Jr. ( JERR-əld; born Leslie Lynch King Jr.; July 14, 1913 – December 26, 2006) was an American politician who served as the 38th president of the United States from 1974 to 1977. He previously served as the leader of the Republican Party in the U.S. House of Representatives from 1965 to 1973, when he was appointed the 40th vice president by President Richard Nixon, after Spiro Agnew's resignation. Ford succeeded to the presidency when Nixon resigned in 1974, but was defeated for election to a full term in 1976. Ford is the only person to become U.S. president without winning an election for president or vice president.\n",
    "Ford was born in Omaha, Nebraska and raised in Grand Rapids, Michigan. He attended the University of Michigan, where he played for the school's football team before eventually attending Yale Law School. Afterward, he served in the U.S. Naval Reserve from 1942 to 1946. Ford began his political career in 1949 as the U.S. representative from Michigan's 5\n",
    "Thought 3: Gerald Ford was born in 1913. Ronald Reagan was born in 1911. 1911 is before 1913. Ronald Reagan was born before Gerald Ford. Answer[SUPPORTS]\"\"\"\n",
    "\n",
    "answer = wiki_react_chain(model,\n",
    "                          parameters,\n",
    "                          context,\n",
    "                          exemplar,\n",
    "                          question,\n",
    "                          show_activity = True)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6d_bgAVJgLa"
   },
   "source": [
    "The limitations of the Wikipedia tool limit the utility of this prompt, as does the lack of support for a neutral \"not enough information\" answer.\n",
    "\n",
    "But consider how easily ReAct adapted to this use case. The ReAct pattern has also shown good results with:\n",
    "* Navigating and interacting with text-based virtual worlds.\n",
    "* Surfing the web.\n",
    "* Using purchasing instructions to make e-commerce transactions.\n",
    "* Conducting a literature search of journal articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHexpQOYLb9E"
   },
   "source": [
    "<a name=\"react-tools\"></a>\n",
    "## Tool Usage Best Practices\n",
    "\n",
    "If you experimented with the prompts above, you probably experienced failures. In many cases, this is because of the limited Wikipedia tool.\n",
    "\n",
    "Following some best practices will help you build more robust and effective tools than in this teaching example.\n",
    "\n",
    "1. **Do** Clearly describe the tool and how to use it in the prompt.\n",
    " * This includes few-shot exemplars demonstrating ideal tool use.\n",
    " * For example, a tool described as \"doc search\" will underperform vs. the same tool described as \"Search internal documents with a natural language query. The response is a list of document names ordered by descending relevancy to the query.\"\n",
    "1. **Do** Carefully consider the scope and complexity of your tools.\n",
    " * **Do** Think through if the API to your tool is simple enough for an LLM to use.\n",
    " * Often multiple simple tools will work better than one complex tool. What a developer sees as a single API may work better as multiple LLM tools.\n",
    " * For example, if your use case requires running SQL to access a database, consider a few separate SQL templates as individual tools vs. using the LLM to generate SQL queries from scratch.\n",
    "1. **Do** Keep the tool output structurally and stylistically consistent.\n",
    " * The less variation in the tool output the more likely the LLM uses the output effectively.\n",
    "1. **Do** Keep tool output short and relevant.\n",
    " * Wordy tool outputs can stress the LLM input length limit.\n",
    " * One great example is the [ReAct paper's Wikipedia agent implementation](https://github.com/ysymyth/ReAct/blob/master/wikienv.py), which includes searching within a Wikipedia article and then only returns a snippet of text around the found term rather than the full article.\n",
    "1. **Do** Handle failures gracefully.\n",
    " * **Do** Catch exceptions and provide useful error messages.\n",
    " * **Do** Manage tool malfunctions like timeouts and rate limits.\n",
    " * **Do** Show error handling in your exemplars.\n",
    "    * If a tool fails and you provide a useful error in the next LLM call, the LLM may self-correct.\n",
    "1. **Do** Tune tool usage prompts.\n",
    " * A parameter-efficient tuning set with a  variety of tool usage (even only 10s of examples) can improve performance significantly.\n",
    "1. **Do** Limit the output length when calling an LLM to generate a tool action.\n",
    " * The LLM will continue generating text beyond the tool action.\n",
    "1. **Don't** Forget about security. Many tool usage patterns create security risks.\n",
    " * **Do** Assume anything accessible via an LLM's tools will be seen by end users experimenting with adversarial inputs.\n",
    " * **Don't** assume your LLM's tool calls will never be malicious. For example, [SQL injection](https://en.wikipedia.org/wiki/SQL_injection) is possible via an LLM tool.\n",
    "\n",
    "The tool in this notebook does not follow many of these best practices.\n",
    "1. Wikipedia articles are unpredictable in structure.\n",
    "1. Wikipedia articles can be 1000s of words but the tool does not support focusing on relevant portions of an article.\n",
    "1. The prompts do not explain what Wikipedia is or how to use it (though the LLM \"knows\" what Wikipedia is from its training data).\n",
    "1. There's no error messages and minimal error handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZyRtBuT_eSQ"
   },
   "source": [
    "## ReAct Advantages\n",
    "\n",
    "1. Fewer hallucinations.\n",
    " * Grounding with a trusted information source vs. relying on an LLM's \"memory\".\n",
    "1. Update/augment LLM knowledge without retraining.\n",
    "1. Works with off-the-shelf LLMs, no additional LLM training or tuning is required.\n",
    "1. Supports a variety of use cases.\n",
    "1. Works with multiple tools.\n",
    "1. Improving overall system performance by improving tools is often easier than improving a prompt or the LLM itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mgJ8MSQKIkt"
   },
   "source": [
    "## ReAct Disadvantages\n",
    "\n",
    "1. Slow (high latency) and expensive, due to  multiple LLM calls.\n",
    "1. External tools mean more system components to maintain and security concerns.\n",
    "1. ReAct loops and other non-answer scenarios are common.\n",
    " * Vs. chain of thought, where hallucinations are more common.\n",
    " * For use cases requiring no specialized or up-to-date information, chain of thought may outperform ReAct.\n",
    "1. ReAct reasoning (think->act) is less flexible and may underperform vs. the more flexible reasoning of pure chain of thought.\n",
    "1. When external information is required, more complex than RAG approaches where the retrieval is not controlled by the LLM.\n",
    "1. Beyond tool integrations, requires additional functionality.\n",
    " * Loop bailouts.\n",
    " * Managing tool errors.\n",
    " * Chain of thought fallbacks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6gBOX-yj6Ar"
   },
   "source": [
    "## ReAct Best Practices\n",
    "\n",
    "Beyond the [tool use](#react-tools) best practices above.\n",
    "\n",
    "1. **Do** Use temperature=0.\n",
    "1. **Don't** Ignore prompt engineering.\n",
    " * How you describe the task and tools can change performance considerably.\n",
    " * **Do** Test exemplars with labels besides \"Thought\", \"Action\", and \"Observation\", along with skipping steps.\n",
    " * **Do** Test exemplars with a variety of thought/reasoning and action styles. For example:\n",
    "    * Some tasks do best with thoughts that identify the next action, other tasks work best when the first thought formulates a complete plan.\n",
    "    * Show thoughts/actions that adjust a plan or reconsider a previous thought after an irrelevant observation or tool error.\n",
    "    * Experiment with thoughts that restate the most salient parts of the prior observation.\n",
    "1. **Do** Catch ReAct chains stuck in a loop.\n",
    " * **Do** Experiment with exemplars showing catching loops.\n",
    " * **Do** Catch repeated actions, and consider returning an observation to the LLM calling out the repeated action--the LLM may be able to recover.\n",
    " * Try rerunning a looping chain with temperature > 0.\n",
    " * When ReAct is the state-of-the-art on a research benchmarking dataset, it's often with a chain of thought self-consistency fallback.\n",
    "1. **Do** Use fine tuning.\n",
    " * **Do** Include tuning examples across the ReAct chain, not just examples of the first or final LLM calls.\n",
    " * **Do** Include error/failure handling in tuning data.\n",
    " * **Don't** Use tuning examples with incorrect ReAct reasoning, even if the final answer is correct.\n",
    "1. **Don't** Implement ReAct without first assessing simpler alternatives.  \n",
    "   * **Do** Consider managed extensions/plugins.\n",
    "     * An extensions service may provide security, observability, monitoring, evaluation, etc., reducing implementation effort.\n",
    "     * **Don't** Assume a managed extensions/plugins service meets your needs without a technical assessment.\n",
    "  * **Do** Consider simpler ways to integrate external knowledge into LLM calls. (i.e., [RAG pattern one above](#rag)).\n",
    "1. **Do** Use an LLM to debug ReAct at scale.\n",
    " * Prompt an LLM to classify failures by type (e.g., reasoning mistake, tool lookup failure, caught in loop) and/or to identify each individual step in the ReAct chain as correct or incorrect.\n",
    "1. **Do** Include tool functionality in tests, performance measurements (including drift), system monitoring, CI/CD, etc.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": ".m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
